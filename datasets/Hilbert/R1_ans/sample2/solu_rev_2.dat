
****************************************
```python
import numpy as np
import matplotlib.pyplot as plt

# ====================================================
# TRIANGULAR SOLVERS (MODULAR IMPLEMENTATION)
# ====================================================

def forward_sub(L, b):
    """Solve Ly = b for lower triangular L using forward substitution"""
    n = L.shape[0]
    y = np.zeros(n)
    for i in range(n):
        y[i] = (b[i] - L[i, :i] @ y[:i]) / L[i, i]
    return y

def backward_sub(U, b):
    """Solve Ux = b for upper triangular U using backward substitution"""
    n = U.shape[0]
    x = np.zeros(n)
    for i in range(n-1, -1, -1):
        x[i] = (b[i] - U[i, i+1:] @ x[i+1:]) / U[i, i]
    return x

# ====================================================
# IMPROVED SOLVER IMPLEMENTATIONS
# ====================================================

def solve_lu(A, b, reg=1e-12):
    """
    Solve Ax = b using LU decomposition with partial pivoting and regularization.
    Technical enhancements:
    - Added Tikhonov regularization to diagonal to stabilize ill-conditioned systems
    - Optimized pivot handling with vectorized row swapping
    - Modular triangular solvers for improved code reuse
    """
    n = A.shape[0]
    A_reg = A + reg * np.eye(n)  # Regularization to diagonal
    U = A_reg.copy().astype(np.float64)
    L = np.eye(n, dtype=np.float64)
    P = np.eye(n)
    
    for k in range(n-1):
        pivot = k + np.argmax(np.abs(U[k:, k]))
        if pivot != k:
            # Vectorized row swapping
            U[[k, pivot], k:] = U[[pivot, k], k:]
            if k > 0:
                L[[k, pivot], :k] = L[[pivot, k], :k]
            P[[k, pivot]] = P[[pivot, k]]
        
        # Pivot magnitude check with regularization fallback
        if np.abs(U[k, k]) < 1e-12:
            U[k, k] += reg
        
        # Vectorized elimination
        factors = U[k+1:, k] / U[k, k]
        L[k+1:, k] = factors
        U[k+1:, k:] -= factors.reshape(-1, 1) * U[k, k:]
    
    Pb = P @ b
    y = forward_sub(L, Pb)
    x = backward_sub(U, y)
    return x

def solve_cholesky(A, b, reg=1e-12):
    """
    Solve Ax = b using Cholesky decomposition with adaptive regularization.
    Technical enhancements:
    - Adaptive regularization based on matrix properties
    - Diagonal safeguard during sqrt operations
    - Optimized using modular triangular solvers
    """
    n = A.shape[0]
    # Adaptive regularization based on min diagonal
    min_diag = np.min(np.diag(A))
    reg = max(reg, abs(min_diag)*1e-3) if min_diag <= 0 else reg
    A_reg = A + reg * np.eye(n)
    L = np.zeros_like(A_reg)
    
    for i in range(n):
        s = L[i, :i] @ L[i, :i]
        diag_val = A_reg[i, i] - s
        # Diagonal safeguard with dynamic adjustment
        if diag_val <= 1e-14:
            diag_val = max(diag_val, 1e-14)
        L[i, i] = np.sqrt(diag_val)
        
        # Vectorized column calculation
        for j in range(i+1, n):
            L[j, i] = (A_reg[j, i] - L[j, :i] @ L[i, :i]) / L[i, i]
    
    y = forward_sub(L, b)
    x = backward_sub(L.T, y)
    return x

def solve_qr(A, b, refine=True):
    """
    Solve Ax = b using QR decomposition via Householder reflections.
    Technical enhancements:
    - Stable sign handling for Householder vectors
    - Optional iterative refinement step
    - Condition number monitoring
    """
    m, n = A.shape
    R = A.copy().astype(np.float64)
    Qb = b.copy().astype(np.float64)
    
    for k in range(n):
        x = R[k:, k]
        if np.linalg.norm(x) < 1e-14:
            continue
            
        # Stable sign handling for Householder vector
        sign = -1 if x[0] < 0 else 1
        e1 = np.zeros_like(x)
        e1[0] = 1
        v = x + sign * np.linalg.norm(x) * e1
        v = v / np.linalg.norm(v)
        
        # Vectorized reflection
        R[k:, k:] -= 2 * np.outer(v, v @ R[k:, k:])
        Qb[k:] -= 2 * v * (v @ Qb[k:])
    
    # Condition number monitoring
    cond = np.linalg.cond(R[:n, :n])
    if cond > 1e10:
        print(f"QR condition number: {cond:.3e}")
    
    # Initial solution
    x = backward_sub(R[:n, :n], Qb[:n])
    
    # Iterative refinement
    if refine:
        r = b - A @ x
        d = backward_sub(R[:n, :n], r[:n])
        x += d
    
    return x

def solve_tikhonov(A, b, lambda_reg=None):
    """
    Solve Ax = b using Tikhonov regularization with adaptive parameter.
    Technical approach:
    - Adaptive lambda based on matrix properties
    - Avoids explicit normal equations formation
    - Uses QR decomposition for numerical stability
    """
    n = A.shape[0]
    # Adaptive regularization parameter
    if lambda_reg is None:
        cond = np.linalg.cond(A)
        lambda_reg = max(1e-12, 1e-3/cond)
    
    # Form augmented system [A; λI]x ≈ [b; 0]
    A_aug = np.vstack((A, np.sqrt(lambda_reg) * np.eye(n)))
    b_aug = np.hstack((b, np.zeros(n)))
    return solve_qr(A_aug, b_aug, refine=False)

def solve_cg(A, b, tol=1e-10, max_iter=None):
    """
    Solve Ax = b using Conjugate Gradient with Jacobi preconditioning.
    Technical enhancements:
    - Jacobi preconditioning for faster convergence
    - Adaptive max iterations based on problem size
    - Relative residual termination criteria
    """
    n = A.shape[0]
    if max_iter is None:
        max_iter = min(500, 2*n)
    
    # Jacobi preconditioner setup
    M_inv = 1 / np.diag(A)
    M_inv = np.where(np.isfinite(M_inv), M_inv, 1.0)
    
    # Initialize variables
    x = np.zeros(n)
    r = b - A @ x
    z = M_inv * r
    p = z.copy()
    rz_old = r @ z
    
    for i in range(max_iter):
        Ap = A @ p
        alpha = rz_old / (p @ Ap)
        x += alpha * p
        r -= alpha * Ap
        rs_norm = np.linalg.norm(r) / np.linalg.norm(b)
        
        # Relative residual check
        if rs_norm < tol:
            break
            
        z = M_inv * r
        rz_new = r @ z
        beta = rz_new / rz_old
        p = z + beta * p
        rz_old = rz_new
    
    return x

# ====================================================
# HILBERT MATRIX CONSTRUCTION (OPTIMIZED)
# ====================================================

def build_hilbert(n):
    """Vectorized Hilbert matrix construction with broadcasting"""
    i = np.arange(1, n+1)
    j = i.reshape(-1, 1)
    return 1.0 / (i + j - 1)

# ====================================================
# ENHANCED SOLUTION COMPARISON FRAMEWORK
# ====================================================

n_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
methods = {
    "LU (reg)": lambda A, b: solve_lu(A, b, reg=1e-12),
    "Cholesky (adaptive)": solve_cholesky,
    "QR (refined)": lambda A, b: solve_qr(A, b, refine=True),
    "Tikhonov (adaptive)": solve_tikhonov,
    "CG (precond)": solve_cg
}

# Enhanced error tracking
errors_Linf = {name: [] for name in methods}
errors_L2 = {name: [] for name in methods}
cond_numbers = []

for n in n_values:
    H = build_hilbert(n)
    x_exact = np.ones(n)
    b = H @ x_exact
    
    # Store condition number for analysis
    cond = np.linalg.cond(H)
    cond_numbers.append(cond)
    print(f"n={n}: cond(H) = {cond:.3e}")
    
    for name, solver in methods.items():
        try:
            x_sol = solver(H, b)
            err_Linf = np.max(np.abs(x_sol - x_exact))
            err_L2 = np.linalg.norm(x_sol - x_exact) / np.linalg.norm(x_exact)
            errors_Linf[name].append(err_Linf)
            errors_L2[name].append(err_L2)
        except Exception as e:
            errors_Linf[name].append(np.nan)
            errors_L2[name].append(np.nan)
            print(f"{name} failed for n={n}: {str(e)}")

# ====================================================
# COMPREHENSIVE RESULTS VISUALIZATION
# ====================================================

plt.figure(figsize=(14, 10))

# L∞ error plot
plt.subplot(2, 1, 1)
for name in methods:
    plt.semilogy(n_values, errors_Linf[name], 'o-', label=name)
plt.title(r'$L_\infty$ Error for Hilbert Matrix Solutions')
plt.xlabel(r'Matrix Size $n$')
plt.ylabel(r'$L_\infty$ Error (log scale)')
plt.grid(True, which='both', linestyle='--', alpha=0.7)
plt.legend()

# Relative L2 error plot
plt.subplot(2, 1, 2)
for name in methods:
    plt.semilogy(n_values, errors_L2[name], 'o-', label=name)
plt.title(r'Relative $L_2$ Error for Hilbert Matrix Solutions')
plt.xlabel(r'Matrix Size $n$')
plt.ylabel(r'Relative $L_2$ Error (log scale)')
plt.grid(True, which='both', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.savefig('hilbert_errors_enhanced.png', dpi=300)

# ====================================================
# STRUCTURED RESULTS OUTPUT
# ====================================================

print("\nDetailed Error Report:")
print("n\tcond(H)\t\t" + "\t".join([f"{name} (L∞)" for name in methods]) + "\t\t" + "\t".join([f"{name} (L2)" for name in methods]))

for i, n in enumerate(n_values):
    err_Linf_strs = [f"{errors_Linf[name][i]:.3e}" for name in methods]
    err_L2_strs = [f"{errors_L2[name][i]:.3e}" for name in methods]
    print(f"{n}\t{cond_numbers[i]:.3e}\t" + "\t".join(err_Linf_strs) + "\t" + "\t".join(err_L2_strs))
```
****************************************
