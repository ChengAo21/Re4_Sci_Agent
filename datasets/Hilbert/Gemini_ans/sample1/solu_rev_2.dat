
****************************************
```python
import numpy as np
import matplotlib.pyplot as plt

# Technical explanation for the solving algorithm:
# This script implements several numerical methods from scratch to solve the linear system H_n x = b,
# where H_n is the n x n Hilbert matrix (h_ij = 1/(i+j-1)) and b is constructed such that the exact
# solution x is a vector of ones. The Hilbert matrix is notoriously ill-conditioned, meaning its
# condition number grows rapidly with n, making accurate solutions challenging due to floating-point
# arithmetic limitations.
#
# The implemented methods are:
# 1. LU Decomposition with Partial Pivoting: A direct method that factorizes A into P L U, where P is a
#    permutation matrix, L is lower triangular with unit diagonal, and U is upper triangular. Partial
#    pivoting is crucial for numerical stability by selecting the largest pivot element in each column
#    to minimize error propagation. The system is solved by forward substitution (Ly = Pb) and backward
#    substitution (Ux = y).
#    - Optimization: Iterative refinement is applied after the initial LU solve to significantly reduce
#      the L-infinity error, especially for ill-conditioned systems like the Hilbert matrix. Crucially,
#      the factorization (LU, P) is performed only once, and these factors are reused to solve for the
#      residual in each refinement step, making the process computationally efficient.
# 2. Cholesky Decomposition: Applicable because the Hilbert matrix is symmetric positive definite (SPD).
#    It factorizes A into L L^T, where L is a lower triangular matrix. This method is generally more
#    numerically stable and efficient for SPD matrices than general LU decomposition. The system is
#    solved by forward substitution (Ly = b) and backward substitution (L^T x = y).
#    - Optimization: A robust diagonal perturbation (jitter) is added during factorization if a diagonal
#      element becomes non-positive due to numerical round-off errors. This jitter is scaled by the
#      matrix's Frobenius norm and machine epsilon, helping to maintain numerical positive definiteness
#      and prevent breakdown for larger 'n' where the Hilbert matrix becomes numerically singular.
#      Despite this, for very large 'n', the extreme ill-conditioning of the Hilbert matrix can still
#      cause Cholesky to fail or produce highly inaccurate results.
# 3. QR Decomposition via Householder Reflections: A direct method that factorizes A into Q R, where Q
#    is an orthogonal matrix and R is an upper triangular matrix. Householder reflections are used to
#    construct Q, ensuring high numerical stability by preserving Euclidean norms. The system is solved
#    by transforming b to Q^T b and then backward substitution (Rx = Q^T b).
#    - Optimization: Similar to LU, iterative refinement is applied to improve the accuracy of the QR
#      solution. The QR factorization is performed once, and the Householder vectors and betas are reused
#      to apply Q^T to the residual in each refinement step, enhancing efficiency.
# 4. Conjugate Gradient (CG) Method with Jacobi Preconditioner: An iterative method suitable for SPD
#    systems. It minimizes the quadratic form associated with the linear system. A Jacobi preconditioner
#    (diagonal scaling) is used to improve the condition number of the system, thereby accelerating
#    convergence, especially for ill-conditioned problems. The iteration stops when the relative residual
#    norm falls below a specified tolerance.
#    - Optimization: The maximum number of iterations for the CG method has been increased to allow for
#      more convergence steps, which is often necessary for highly ill-conditioned systems to reach the
#      desired tolerance. While Jacobi preconditioning is simple, more advanced preconditioners like
#      Incomplete Cholesky or SSOR could further improve convergence for such ill-conditioned matrices.
# 5. Tikhonov Regularization: A technique to stabilize the solution of ill-conditioned systems. It
#    transforms the original system H_n x = b into a regularized normal equation (H_n^T H_n + lambda I)x = H_n^T b.
#    The addition of a small positive lambda * I (regularization parameter times identity matrix) makes
#    the new system matrix better conditioned. This regularized system is then solved using Cholesky
#    decomposition, as (H_n^T H_n + lambda I) is symmetric positive definite.
#    - Note: The regularization parameter lambda is fixed in this implementation. In practice, lambda can
#      be tuned using methods like the L-curve or Generalized Cross-Validation (GCV) to balance bias
#      (introduced by regularization) and variance (due to noise/ill-conditioning). Tikhonov regularization
#      provides a stable solution but introduces a bias, leading to a plateau in the error for increasing 'n'.

# Helper functions
def generate_hilbert_matrix(n):
    """
    Generates an n x n Hilbert matrix.
    h_ij = 1 / (i + j - 1) for 1-indexed i,j. For 0-indexed i,j it's 1 / (i + j + 1).
    """
    H = np.zeros((n, n), dtype=np.float64)
    for i in range(n):
        for j in range(n):
            H[i, j] = 1.0 / (i + j + 1.0)
    return H

def generate_b_vector(H_n):
    """
    Generates the right-hand side vector b such that the exact solution x is a vector of ones.
    b = H_n * x_exact, where x_exact = (1, 1, ..., 1)^T
    """
    n = H_n.shape[0]
    x_exact = np.ones(n, dtype=np.float64)
    b = np.dot(H_n, x_exact)
    return b, x_exact

def calculate_l_infinity_error(computed_x, exact_x):
    """
    Calculates the L-infinity norm (maximum absolute difference) between two vectors.
    """
    return np.max(np.abs(computed_x - exact_x))

def calculate_residual_norm(A, b, x_computed):
    """
    Calculates the L-infinity norm of the residual: ||b - A * x_computed||_inf.
    """
    return np.linalg.norm(b - np.dot(A, x_computed), ord=np.inf)

# Optimization: Iterative Refinement function
def iterative_refinement(A, b, x0, solver_func, factors=None, max_iter=5, tol=1e-12):
    """
    Improves the solution x0 for Ax=b using iterative refinement.
    solver_func: A function that takes (factors, rhs) and returns the solution vector.
                 'factors' are the pre-computed decomposition factors (e.g., LU, P for LU; R, v, beta for QR).
    Optimization: This function now reuses the pre-computed factors for solving the residual system,
                  significantly improving efficiency compared to re-factorizing in each step.
    """
    x_current = x0.copy()
    
    initial_b_norm = np.linalg.norm(b, ord=np.inf)
    if initial_b_norm == 0:
        return x_current # If b is zero, x is zero.

    for i in range(max_iter):
        # Compute residual r = b - A * x_current
        r = b - np.dot(A, x_current)
        
        # Check if residual is already small enough
        if np.linalg.norm(r, ord=np.inf) < tol * initial_b_norm:
            break
        
        # Solve A * delta_x = r using the provided solver function and pre-computed factors
        try:
            delta_x = solver_func(factors, r)
        except Exception as e:
            # If the solver fails during refinement, stop.
            # print(f"  Iterative refinement failed to solve for delta_x: {e}")
            break
        
        # Update solution x_current = x_current + delta_x
        x_current = x_current + delta_x
        
        # Check for convergence of delta_x (relative to current solution)
        if np.linalg.norm(delta_x, ord=np.inf) < tol * np.linalg.norm(x_current, ord=np.inf):
            break
            
    return x_current

# 1. LU Decomposition with Partial Pivoting (from scratch)
def lu_factor_pivot(A):
    """
    Performs LU factorization with partial pivoting on matrix A.
    Returns (LU, P) where LU contains L and U factors, and P is the permutation vector.
    """
    n = A.shape[0]
    LU = A.copy()
    P = np.arange(n) # Permutation vector, initially identity

    for k in range(n - 1):
        # Find pivot row: row with the largest absolute value in the current column (from k downwards)
        pivot_row_relative = np.argmax(np.abs(LU[k:, k]))
        pivot_row_absolute = k + pivot_row_relative
        
        # Swap rows in LU matrix and permutation vector if necessary
        if pivot_row_absolute != k:
            LU[[k, pivot_row_absolute], :] = LU[[pivot_row_absolute, k], :]
            P[[k, pivot_row_absolute]] = P[[pivot_row_absolute, k]]
        
        # Check for zero pivot after potential swap
        if LU[k, k] == 0:
            raise ValueError(f"Matrix is singular or numerically singular at pivot {k}.")
        
        # Compute multipliers and perform elimination
        multipliers = LU[k+1:, k] / LU[k, k]
        LU[k+1:, k] = multipliers # Store multipliers in the lower triangular part (L)
        LU[k+1:, k+1:] -= np.outer(multipliers, LU[k, k+1:]) # Update the trailing submatrix
            
    return LU, P

def lu_solve_pivot(factors, b):
    """
    Solves the linear system Ax = b using LU factors (LU, P) obtained from lu_factor_pivot.
    A is implicitly P^T L U.
    factors: tuple (LU, P)
    """
    LU, P = factors
    n = LU.shape[0]
    
    # Apply permutation to b: Pb
    b_perm = b[P]
    
    # Forward substitution: Ly = Pb
    # L is implicitly stored in LU (unit diagonal, multipliers below diagonal)
    y = np.zeros(n, dtype=np.float64)
    for i in range(n):
        y[i] = b_perm[i] - np.dot(LU[i, :i], y[:i]) # LU[i, :i] contains L_i,j for j < i
    
    # Backward substitution: Ux = y
    # U is implicitly stored in LU (upper triangular part)
    x = np.zeros(n, dtype=np.float64)
    for i in range(n - 1, -1, -1):
        if LU[i, i] == 0:
            raise ValueError(f"U matrix has a zero diagonal element at {i}, cannot solve.")
        x[i] = (y[i] - np.dot(LU[i, i+1:], x[i+1:])) / LU[i, i] # LU[i, i+1:] contains U_i,j for j > i
        
    return x

# 2. Cholesky Decomposition (from scratch)
def cholesky_factor(A):
    """
    Performs Cholesky factorization on a symmetric positive definite matrix A.
    Returns L such that A = L L^T.
    Optimization: Added a robust epsilon for numerical stability to handle near-zero or negative diagonal elements.
                  The epsilon is scaled by the matrix's Frobenius norm and machine epsilon.
    """
    n = A.shape[0]
    L = np.zeros((n, n), dtype=np.float64)
    
    # Calculate a robust epsilon based on matrix norm and machine epsilon
    # This helps to prevent breakdown for ill-conditioned matrices by ensuring diagonal elements are positive.
    robust_epsilon = np.finfo(A.dtype).eps * np.linalg.norm(A, ord='fro') * n 

    for j in range(n):
        # Calculate diagonal element L[j, j]
        # L[j,j]^2 = A[j,j] - sum(L[j,k]^2 for k=0 to j-1)
        sum_k_sq = np.dot(L[j, :j], L[j, :j])
        val = A[j, j] - sum_k_sq
        
        # Optimization: Add a small perturbation if val is non-positive due to numerical errors
        # This ensures that np.sqrt does not receive a negative number and helps maintain
        # numerical positive definiteness for ill-conditioned matrices.
        if val <= 0:
            val = max(val, robust_epsilon) # Ensure val is at least robust_epsilon
            if val <= 0: # If still non-positive (e.g., A[j,j] was extremely negative), raise error
                raise ValueError(f"Matrix is not positive definite or numerically singular at diagonal {j}.")
            L[j, j] = np.sqrt(val)
        else:
            L[j, j] = np.sqrt(val)

        # Calculate off-diagonal elements L[i, j] for i > j
        # L[i,j] = (A[i,j] - sum(L[i,k]*L[j,k] for k=0 to j-1)) / L[j,j]
        for i in range(j + 1, n):
            sum_k_prod = np.dot(L[i, :j], L[j, :j])
            L[i, j] = (A[i, j] - sum_k_prod) / L[j, j]
            
    return L

def cholesky_solve(L, b):
    """
    Solves the linear system Ax = b using Cholesky factor L (A = L L^T).
    """
    n = L.shape[0]
    
    # Forward substitution: Ly = b
    y = np.zeros(n, dtype=np.float64)
    for i in range(n):
        if L[i, i] == 0:
            raise ValueError(f"L matrix has a zero diagonal element at {i}, cannot solve.")
        y[i] = (b[i] - np.dot(L[i, :i], y[:i])) / L[i, i]
        
    # Backward substitution: L^T x = y
    x = np.zeros(n, dtype=np.float64)
    LT = L.T # Transpose of L
    for i in range(n - 1, -1, -1):
        if LT[i, i] == 0:
            raise ValueError(f"L^T matrix has a zero diagonal element at {i}, cannot solve.")
        x[i] = (y[i] - np.dot(LT[i, i+1:], x[i+1:])) / LT[i, i]
        
    return x

# 3. QR Decomposition via Householder Reflections (from scratch)
def qr_factor_householder(A):
    """
    Performs QR factorization of matrix A using Householder reflections.
    Returns (R, householder_vectors, betas) where R is the upper triangular matrix,
    and householder_vectors/betas implicitly define Q.
    """
    n, m = A.shape
    R = A.copy()
    householder_vectors = [] # Stores v for each reflection
    betas = [] # Stores beta for each reflection (2 / ||v||^2)

    for k in range(min(n, m)):
        # Extract the column vector to be zeroed out below the diagonal
        x = R[k:, k]
        
        # Compute Householder vector v
        # alpha = -sign(x_0) * ||x||_2
        alpha = -np.copysign(np.linalg.norm(x), x[0])
        v = x.copy()
        v[0] = v[0] - alpha # v = x - alpha * e_1
        
        # Compute beta (2 / ||v||^2)
        norm_v_sq = np.dot(v, v)
        if norm_v_sq == 0: # Avoid division by zero if x is already zero (e.g., column is already zeroed)
            beta = 0.0
        else:
            beta = 2.0 / norm_v_sq
        
        householder_vectors.append(v)
        betas.append(beta)
        
        # Apply Householder reflection to the rest of the matrix R
        # P = I - beta * v v^T
        # R_new = P R_old
        # R_new[k:, k:] = R_old[k:, k:] - beta * v (v^T R_old[k:, k:])
        R[k:, k:] -= beta * np.outer(v, np.dot(v.T, R[k:, k:]))
        
        # Set elements below diagonal to zero for numerical cleanup (they should be zero theoretically)
        if k < n - 1:
            R[k+1:, k] = 0.0
            
    return R, householder_vectors, betas

def qr_solve_householder(factors, b):
    """
    Solves Ax = b using QR factorization via Householder reflections.
    factors: tuple (R, householder_vectors, betas) obtained from qr_factor_householder.
    """
    R, householder_vectors, betas = factors
    n = R.shape[0] # Assuming square matrix for solving Ax=b
    
    # Apply Q^T to b (Q^T b = (P_n-1 ... P_1) b)
    # Each P_k = I - beta_k v_k v_k^T
    # P_k b = b - beta_k v_k (v_k^T b)
    b_transformed = b.copy()
    for k in range(len(householder_vectors)):
        v = householder_vectors[k]
        beta = betas[k]
        
        # Apply reflection to the relevant part of b_transformed (from index k downwards)
        # Note: v is already sliced to be of appropriate length (n-k)
        b_transformed[k:] -= beta * np.dot(v, b_transformed[k:]) * v
        
    # Backward substitution: Rx = Q^T b
    x = np.zeros(n, dtype=np.float64)
    for i in range(n - 1, -1, -1):
        if R[i, i] == 0:
            raise ValueError(f"R matrix has a zero diagonal element at {i}, cannot solve.")
        x[i] = (b_transformed[i] - np.dot(R[i, i+1:], x[i+1:])) / R[i, i]
        
    return x

# 4. Conjugate Gradient (CG) Method with Jacobi Preconditioner (from scratch)
def conjugate_gradient(A, b, tol=1e-9, max_iter=1000, preconditioner=None):
    """
    Solves Ax = b using the Conjugate Gradient method.
    A must be symmetric positive definite.
    preconditioner: a function M_inv(r) that applies the inverse of the preconditioner M to r.
    Optimization: Increased max_iter to allow for better convergence on ill-conditioned systems.
    """
    n = A.shape[0]
    x = np.zeros(n, dtype=np.float64)
    r = b - np.dot(A, x) # Initial residual
    
    # Apply preconditioner to initial residual
    if preconditioner:
        z = preconditioner(r)
    else:
        z = r.copy()
        
    p = z.copy() # Initial search direction
    
    r_dot_z = np.dot(r, z) # r_k^T z_k
    
    initial_b_norm = np.linalg.norm(b)
    if initial_b_norm == 0: # Handle case where b is zero vector
        return x

    for i in range(max_iter):
        Ap = np.dot(A, p) # A * p_k
        
        # Check for breakdown (p^T A p close to zero)
        p_dot_Ap = np.dot(p, Ap)
        if p_dot_Ap <= 0: # Should be positive for SPD matrix
            # print(f"Warning: CG breakdown at iteration {i} due to p^T A p <= 0. Returning current solution.")
            return x
            
        alpha = r_dot_z / p_dot_Ap # Step size alpha_k
        
        x_new = x + alpha * p # Update solution x_{k+1}
        r_new = r - alpha * Ap # Update residual r_{k+1}
        
        # Check convergence using relative residual norm
        if np.linalg.norm(r_new) < tol * initial_b_norm:
            return x_new
        
        # Apply preconditioner to new residual
        if preconditioner:
            z_new = preconditioner(r_new)
        else:
            z_new = r_new.copy()
            
        r_dot_z_new = np.dot(r_new, z_new) # r_{k+1}^T z_{k+1}
        
        # Check for breakdown (r_dot_z close to zero)
        if r_dot_z_new <= 0: # Should be positive for SPD matrix
            # print(f"Warning: CG breakdown at iteration {i} due to r^T z <= 0. Returning current solution.")
            return x_new

        beta = r_dot_z_new / r_dot_z # Update parameter beta_k
        
        p_new = z_new + beta * p # Update search direction p_{k+1}
        
        # Update for next iteration
        x = x_new
        r = r_new
        z = z_new
        p = p_new
        r_dot_z = r_dot_z_new
        
    # If max_iter reached without convergence
    # print(f"Warning: CG did not converge within {max_iter} iterations. Final relative residual norm: {np.linalg.norm(r) / initial_b_norm:.2e}")
    return x

def jacobi_preconditioner(A_diag):
    """
    Returns a function that applies the inverse of the Jacobi preconditioner.
    M_inv(r) = diag(A)^-1 * r
    A_diag is the diagonal of the matrix A.
    Handles zero diagonal elements for robustness.
    """
    inv_diag = np.zeros_like(A_diag)
    non_zero_mask = A_diag != 0
    inv_diag[non_zero_mask] = 1.0 / A_diag[non_zero_mask]
    
    def M_inv(r):
        return inv_diag * r
    return M_inv

# 5. Tikhonov Regularization (from scratch, using Cholesky for the regularized system)
def tikhonov_regularization_solve(H_n, b, lambda_val):
    """
    Solves the regularized system (H_n^T H_n + lambda I)x = H_n^T b.
    Uses the implemented Cholesky decomposition for the solution.
    """
    n = H_n.shape[0]
    
    # Form the regularized system matrix A_reg = H_n^T H_n + lambda I
    A_reg = np.dot(H_n.T, H_n) + lambda_val * np.eye(n, dtype=np.float64)
    
    # Form the regularized right-hand side b_reg = H_n^T b
    b_reg = np.dot(H_n.T, b)
    
    # Solve using Cholesky decomposition (A_reg is symmetric positive definite)
    # Uses the modified cholesky_factor with robust epsilon for robustness
    L_reg = cholesky_factor(A_reg)
    x_tikhonov = cholesky_solve(L_reg, b_reg)
    
    return x_tikhonov

# Main execution block
if __name__ == "__main__":
    n_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
    
    # Store errors and residual norms for plotting
    errors_lu = []
    residuals_lu = []
    errors_cholesky = []
    residuals_cholesky = []
    errors_qr = []
    residuals_qr = []
    errors_cg = []
    residuals_cg = []
    errors_tikhonov = []
    residuals_tikhonov = []

    print("Solving H_n x = b for various n values and comparing L_infinity errors and residual norms.")
    print("-" * 100)

    for n in n_values:
        print(f"\nProcessing n = {n}")
        H_n = generate_hilbert_matrix(n)
        b, x_exact = generate_b_vector(H_n)

        # --- LU Decomposition with Partial Pivoting ---
        try:
            # Optimization: Factorize once outside the iterative refinement loop
            LU_factors, P_factors = lu_factor_pivot(H_n)
            
            # Define a wrapper for lu_solve_pivot that uses pre-computed factors
            def lu_solver_wrapper_for_refinement(factors_tuple, rhs):
                return lu_solve_pivot(factors_tuple, rhs)

            x_lu_initial = lu_solve_pivot((LU_factors, P_factors), b)
            # Optimization: Apply iterative refinement, reusing the factors
            x_lu = iterative_refinement(H_n, b, x_lu_initial, lu_solver_wrapper_for_refinement, factors=(LU_factors, P_factors))
            
            error_lu = calculate_l_infinity_error(x_lu, x_exact)
            residual_lu = calculate_residual_norm(H_n, b, x_lu)
            errors_lu.append(error_lu)
            residuals_lu.append(residual_lu)
            print(f"  LU (Pivoting) Error: {error_lu:.2e}, Residual: {residual_lu:.2e}")
        except Exception as e:
            errors_lu.append(np.nan) # Mark as NaN if error occurs
            residuals_lu.append(np.nan)
            print(f"  LU (Pivoting) Error: Failed - {e}")

        # --- Cholesky Decomposition ---
        try:
            # Optimization: cholesky_factor now includes robust epsilon for robustness against numerical issues
            L_cholesky = cholesky_factor(H_n)
            x_cholesky = cholesky_solve(L_cholesky, b)
            error_cholesky = calculate_l_infinity_error(x_cholesky, x_exact)
            residual_cholesky = calculate_residual_norm(H_n, b, x_cholesky)
            errors_cholesky.append(error_cholesky)
            residuals_cholesky.append(residual_cholesky)
            print(f"  Cholesky Error:      {error_cholesky:.2e}, Residual: {residual_cholesky:.2e}")
        except Exception as e:
            errors_cholesky.append(np.nan)
            residuals_cholesky.append(np.nan)
            print(f"  Cholesky Error:      Failed - {e}")

        # --- QR Decomposition via Householder Reflections ---
        try:
            # Optimization: Factorize once outside the iterative refinement loop
            R_factors, householder_vectors_factors, betas_factors = qr_factor_householder(H_n)
            
            # Define a wrapper for qr_solve_householder that uses pre-computed factors
            def qr_solver_wrapper_for_refinement(factors_tuple, rhs):
                return qr_solve_householder(factors_tuple, rhs)

            x_qr_initial = qr_solve_householder((R_factors, householder_vectors_factors, betas_factors), b)
            # Optimization: Apply iterative refinement, reusing the factors
            x_qr = iterative_refinement(H_n, b, x_qr_initial, qr_solver_wrapper_for_refinement, factors=(R_factors, householder_vectors_factors, betas_factors))
            
            error_qr = calculate_l_infinity_error(x_qr, x_exact)
            residual_qr = calculate_residual_norm(H_n, b, x_qr)
            errors_qr.append(error_qr)
            residuals_qr.append(residual_qr)
            print(f"  QR (Householder) Error: {error_qr:.2e}, Residual: {residual_qr:.2e}")
        except Exception as e:
            errors_qr.append(np.nan)
            residuals_qr.append(np.nan)
            print(f"  QR (Householder) Error: Failed - {e}")

        # --- Conjugate Gradient with Jacobi Preconditioner ---
        # CG tolerance is set relative to the initial norm of b.
        # Optimization: Increased max iterations for better convergence on ill-conditioned systems.
        cg_tol = 1e-10 
        cg_max_iter = 50 * n # Increased from 10*n to 50*n for more convergence steps

        try:
            # Prepare Jacobi preconditioner
            A_diag = np.diag(H_n)
            jacobi_M_inv = jacobi_preconditioner(A_diag)
            
            x_cg = conjugate_gradient(H_n, b, tol=cg_tol, max_iter=cg_max_iter, preconditioner=jacobi_M_inv)
            error_cg = calculate_l_infinity_error(x_cg, x_exact)
            residual_cg = calculate_residual_norm(H_n, b, x_cg)
            errors_cg.append(error_cg)
            residuals_cg.append(residual_cg)
            print(f"  CG (Jacobi Prec) Error: {error_cg:.2e}, Residual: {residual_cg:.2e}")
        except Exception as e:
            errors_cg.append(np.nan)
            residuals_cg.append(np.nan)
            print(f"  CG (Jacobi Prec) Error: Failed - {e}")

        # --- Tikhonov Regularization ---
        # Lambda choice is critical for Tikhonov regularization. A small fixed value is used.
        # For Hilbert matrices, condition numbers grow extremely fast, so even a small lambda
        # can significantly improve stability at the cost of introducing a slight bias.
        # Note: lambda is a tunable parameter. For optimal results, adaptive selection methods
        # like L-curve or GCV should be used.
        lambda_tikhonov = 1e-10 
        try:
            # Uses the modified cholesky_factor internally for robustness
            x_tikhonov = tikhonov_regularization_solve(H_n, b, lambda_tikhonov)
            error_tikhonov = calculate_l_infinity_error(x_tikhonov, x_exact)
            residual_tikhonov = calculate_residual_norm(H_n, b, x_tikhonov)
            errors_tikhonov.append(error_tikhonov)
            residuals_tikhonov.append(residual_tikhonov)
            print(f"  Tikhonov (lambda={lambda_tikhonov:.0e}) Error: {error_tikhonov:.2e}, Residual: {residual_tikhonov:.2e}")
        except Exception as e:
            errors_tikhonov.append(np.nan)
            residuals_tikhonov.append(np.nan)
            print(f"  Tikhonov (lambda={lambda_tikhonov:.0e}) Error: Failed - {e}")
            
    print("-" * 100)
    print("\nSummary of L_infinity Errors and Residual Norms:")
    print(f"{'n':<3} | {'LU Error':<12} {'LU Residual':<15} | {'Cholesky Error':<16} {'Cholesky Residual':<19} | {'QR Error':<12} {'QR Residual':<15} | {'CG Error':<10} {'CG Residual':<13} | {'Tikhonov Error':<16} {'Tikhonov Residual':<19}")
    print("-" * 150)
    for i, n_val in enumerate(n_values):
        lu_err = f"{errors_lu[i]:.2e}" if not np.isnan(errors_lu[i]) else "Failed"
        lu_res = f"{residuals_lu[i]:.2e}" if not np.isnan(residuals_lu[i]) else "Failed"
        chol_err = f"{errors_cholesky[i]:.2e}" if not np.isnan(errors_cholesky[i]) else "Failed"
        chol_res = f"{residuals_cholesky[i]:.2e}" if not np.isnan(residuals_cholesky[i]) else "Failed"
        qr_err = f"{errors_qr[i]:.2e}" if not np.isnan(errors_qr[i]) else "Failed"
        qr_res = f"{residuals_qr[i]:.2e}" if not np.isnan(residuals_qr[i]) else "Failed"
        cg_err = f"{errors_cg[i]:.2e}" if not np.isnan(errors_cg[i]) else "Failed"
        cg_res = f"{residuals_cg[i]:.2e}" if not np.isnan(residuals_cg[i]) else "Failed"
        tikh_err = f"{errors_tikhonov[i]:.2e}" if not np.isnan(errors_tikhonov[i]) else "Failed"
        tikh_res = f"{residuals_tikhonov[i]:.2e}" if not np.isnan(residuals_tikhonov[i]) else "Failed"
        
        print(f"{n_val:<3} | {lu_err:<12} {lu_res:<15} | {chol_err:<16} {chol_res:<19} | {qr_err:<12} {qr_res:<15} | {cg_err:<10} {cg_res:<13} | {tikh_err:<16} {tikh_res:<19}")

    # Plotting the results (L-infinity Error)
    plt.figure(figsize=(14, 9))
    plt.semilogy(n_values, errors_lu, 'o-', label='LU with Partial Pivoting (with Iterative Refinement)')
    plt.semilogy(n_values, errors_cholesky, 's-', label='Cholesky Decomposition (Robust)')
    plt.semilogy(n_values, errors_qr, '^-', label='QR Decomposition (Householder, with Iterative Refinement)')
    plt.semilogy(n_values, errors_cg, 'x-', label='Conjugate Gradient (Jacobi Prec, increased max_iter)')
    plt.semilogy(n_values, errors_tikhonov, 'd-', label=f'Tikhonov Regularization (λ={lambda_tikhonov:.0e})')

    plt.xlabel('Matrix Size (n)')
    plt.ylabel('L-infinity Error (log scale)')
    plt.title('L-infinity Error vs. Matrix Size for Solving Hilbert System')
    plt.legend()
    plt.grid(True, which="both", ls="-")
    plt.xticks(n_values)
    plt.show()

    # Plotting Residual Norms
    plt.figure(figsize=(14, 9))
    plt.semilogy(n_values, residuals_lu, 'o--', label='LU with Partial Pivoting (Residual)')
    plt.semilogy(n_values, residuals_cholesky, 's--', label='Cholesky Decomposition (Residual)')
    plt.semilogy(n_values, residuals_qr, '^--', label='QR Decomposition (Residual)')
    plt.semilogy(n_values, residuals_cg, 'x--', label='Conjugate Gradient (Residual)')
    plt.semilogy(n_values, residuals_tikhonov, 'd--', label=f'Tikhonov Regularization (Residual, λ={lambda_tikhonov:.0e})')

    plt.xlabel('Matrix Size (n)')
    plt.ylabel('L-infinity Residual Norm (log scale)')
    plt.title('L-infinity Residual Norm vs. Matrix Size for Solving Hilbert System (Residual Norms)')
    plt.legend()
    plt.grid(True, which="both", ls="-")
    plt.xticks(n_values)
    plt.show()
```
****************************************
