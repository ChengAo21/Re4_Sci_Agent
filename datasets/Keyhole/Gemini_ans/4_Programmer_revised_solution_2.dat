
****************************************
```python
# Imports
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import scipy.linalg
import sys # For graceful exit
from joblib import Parallel, delayed # For parallel processing

# Technical explanation for the solving algorithm
# The problem aims to identify an optimal dimensionless quantity (Pi) formed by combining seven physical parameters,
# such that Pi exhibits the highest coefficient of determination (R-squared) with the keyhole aspect ratio (e*).
# The exponents of the physical quantities in Pi must be integers or rational fractions, with absolute values not exceeding 3,
# and the resulting dimensionless exponents must be normalized by the laser scan speed (V_s).

# Algorithm: Systematic Dimensional Analysis with Fixed V_s Exponent and Optimized Grid Search
# 1.  **Dimensional Analysis Setup**:
#     -   Define the fundamental dimensions (Mass [M], Length [L], Time [T], Temperature [Theta]) for each of the seven physical quantities:
#         eta P (Power): [M L^2 T^-3]
#         V_s (Speed): [L T^-1]
#         r_0 (Length): [L]
#         alpha (Thermal Diffusivity): [L^2 T^-1]
#         rho (Density): [M L^-3]
#         C_p (Heat Capacity): [L^2 T^-2 Theta^-1]
#         T_l-T_0 (Temperature Difference): [Theta]
#     -   Construct a dimensional matrix (D) where rows represent fundamental dimensions and columns represent physical quantities.
#         For a dimensionless quantity Pi = Q1^x1 * Q2^x2 * ... * Qn^xn, the sum of (exponent * dimension_vector) for each fundamental dimension must be zero.
#         This translates to D @ x = 0, where x is the vector of exponents.
# 2.  **Exponent Constraints and Normalization**:
#     -   The problem requires exponents to be integers or rational fractions within [-3, 3]. A predefined set of rational numbers (e.g., integers and half-integers, quarters, thirds) is generated for the grid search.
#     -   "Normalized by V_s" implies that the exponent of V_s (let's call it 'b') should be a fixed reference, typically 1 or -1. This significantly reduces the search space.
# 3.  **Reduced Search Space via Dimensional Homogeneity**:
#     -   The dimensional matrix D (4x7) implies that 4 exponents can be expressed as linear combinations of the remaining 3.
#     -   By fixing the exponent of V_s (b) to either 1 or -1, we are left with 2 independent exponents to iterate over (e.g., r_0 and alpha). The remaining 4 exponents (eta P, rho, C_p, T_l-T_0) can then be determined by solving the system of linear equations derived from D @ x = 0.
#     -   Specifically, we partition the exponent vector `x` into `x_independent` (e.g., `r_0`, `alpha`) and `x_dependent` (e.g., `eta P`, `rho`, `C_p`, `T_l-T_0`), with `V_s`'s exponent `b` being fixed. The equation becomes `D_dep @ x_dep = -D_ind @ x_ind - D_b * b_fixed`.
# 4.  **Optimized Grid Search and R-squared Evaluation**:
#     -   Instead of nested Python loops for independent exponents, all combinations of independent exponents are generated using `numpy.meshgrid`.
#     -   The calculation of dependent exponents is vectorized using NumPy's array operations, significantly speeding up this part of the search.
#     -   The rounding and validation of dependent exponents are also performed in a vectorized manner.
#     -   For each *valid* combination of exponents, the dimensionless quantity Pi is constructed for all data points: `Pi = (eta P)^a * (V_s)^b * ...`.
#     -   To evaluate the correlation, a linear regression is performed between `log(e*)` and `log(Pi)`. This is because a power-law relationship `e* = K * Pi^m` transforms into a linear relationship `log(e*) = log(K) + m * log(Pi)`.
#     -   The R-squared value from this linear regression is calculated using K-Fold Cross-Validation to ensure robustness and generalization. Root Mean Squared Error (RMSE) is also calculated as an additional diagnostic.
#     -   **Optimization**: The evaluation of each valid dimensionless group (i.e., performing the linear regression and calculating metrics) is parallelized using `joblib` to leverage multiple CPU cores, significantly reducing the total execution time for the grid search.
# 5.  **Optimal Group Identification**:
#     -   The set of exponents that yields the highest R-squared value (averaged from cross-validation) is identified as the optimal dimensionless quantity.
# 6.  **Result Reporting and Visualization**:
#     -   The optimal R-squared, RMSE, the formula for the optimal dimensionless quantity, and the corresponding exponents are printed.
#     -   Plots of `log(e*)` vs `log(Pi_optimal)` and `e*` vs `Pi_optimal` are generated and saved to visualize the correlation.
#     -   `plt.close()` is added after saving figures to prevent memory warnings.

# Constants and configurations
FILE_PATH = 'Q:/Work2/LLM_/test_Agent/dataset_keyhole.csv'
# Column indices (0-indexed) for the physical quantities
# eta P (col 3), V_s (col 4), r_0 (col 5), alpha (col 6), rho (col 7), C_p (col 8), T_l-T_0 (col 11)
# e* (last column)
COL_INDICES = [2, 3, 4, 5, 6, 7, 10] # Corresponding to etaP, Vs, r0, alpha, rho, Cp, Tl-T0
QUANTITY_NAMES = ['etaP', 'Vs', 'r0', 'alpha', 'rho', 'Cp', 'Tl-T0']
TARGET_COL_INDEX = -1 # e*

# Define the dimensional matrix D for [M, L, T, Theta]
# Quantities order: [etaP, Vs, r0, alpha, rho, Cp, Tl-T0]
# Dimensions:
# M: Mass
# L: Length
# T: Time
# Theta: Temperature
DIMENSIONAL_MATRIX = np.array([
    # etaP Vs r0 alpha rho Cp Tl-T0
    [1, 0, 0, 0, 1, 0, 0],  # M
    [2, 1, 1, 2, -3, 2, 0], # L
    [-3, -1, 0, -1, 0, -2, 0], # T
    [0, 0, 0, 0, 0, -1, 1]  # Theta
])

# Tolerance for floating point comparisons when rounding exponents (relaxed from 1e-5 as per feedback)
EXPONENT_TOLERANCE = 1e-4
MAX_ABS_EXPONENT = 3 # Maximum absolute value allowed for exponents
# Denominators to consider for rational exponents (made configurable as per feedback)
POSSIBLE_DENOMINATORS = [1, 2, 3, 4]
# Number of folds for K-Fold Cross-Validation
N_SPLITS_CV = 5

# Helper functions for modularity and clarity

def load_and_preprocess_data(file_path, col_indices, target_col_index):
    """
    Loads data from CSV, extracts relevant columns, and preprocesses for log-log regression.
    Filters out rows with non-positive values.

    Args:
        file_path (str): Path to the CSV data file.
        col_indices (list): List of 0-indexed column indices for physical quantities.
        target_col_index (int): 0-indexed column index for the target variable (e*).

    Returns:
        tuple: (log_X_data, log_y_data, X_data_positive, y_data_positive)
               log-transformed input features, log-transformed target, original positive features, original positive target.
    """
    # Ensure only use # for annotations to explain each section of the code
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}. Please check the path.")
        sys.exit(1) # Exit gracefully if file is not found
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        sys.exit(1) # Exit gracefully for other file reading errors

    # Extract relevant columns
    X_data = df.iloc[:, col_indices].values
    y_data = df.iloc[:, target_col_index].values

    # Filter out rows where any value is non-positive before log transformation.
    # Physical quantities are expected to be positive. Filtering is preferred over adding epsilon
    # to avoid introducing artificial values or masking true zeros/errors in data,
    # unless the problem explicitly states that zero values should be treated as small positives.
    initial_rows = X_data.shape[0]
    positive_mask = (X_data > 0).all(axis=1) & (y_data > 0)
    X_data_positive = X_data[positive_mask]
    y_data_positive = y_data[positive_mask]

    if X_data_positive.shape[0] == 0:
        print("Error: No positive data points found for log transformation. Exiting.")
        sys.exit(1) # Exit if no valid data points remain
    elif X_data_positive.shape[0] < initial_rows:
        # Warning message for discarded data points
        print(f"Warning: {initial_rows - X_data_positive.shape[0]} rows discarded due to non-positive values.")

    log_X_data = np.log(X_data_positive)
    log_y_data = np.log(y_data_positive)

    return log_X_data, log_y_data, X_data_positive, y_data_positive

def generate_possible_exponents(max_abs_val, denominators):
    """
    Generates a sorted array of possible rational exponents within a given range.

    Args:
        max_abs_val (int): Maximum absolute value for exponents.
        denominators (list): List of denominators to consider for rational fractions.

    Returns:
        np.ndarray: Sorted array of unique possible exponents.
    """
    # Ensure only use # for annotations to explain each section of the code
    # Generate raw possible exponents (e.g., k/d)
    # k ranges from -max_abs_val * max_d to max_abs_val * max_d to ensure all fractions up to max_abs_val are covered
    max_d = max(denominators)
    possible_exponents_raw = np.array([k/d for d in denominators for k in range(-max_abs_val * max_d, max_abs_val * max_d + 1)])
    # Filter to ensure absolute value is within max_abs_val and get unique values
    possible_exponents = np.unique(possible_exponents_raw[np.abs(possible_exponents_raw) <= max_abs_val])
    possible_exponents.sort() # Sort for consistent iteration
    return possible_exponents

def round_and_validate_exponents_batch(x_dep_raw_batch, possible_exponents, tolerance, max_abs_val):
    """
    Rounds a batch of raw dependent exponents to the nearest rational values
    and validates them against tolerance and absolute bounds.

    Args:
        x_dep_raw_batch (np.ndarray): Raw dependent exponents (num_dependent_exponents, N_combinations).
        possible_exponents (np.ndarray): Array of allowed rational exponents.
        tolerance (float): Tolerance for rounding.
        max_abs_val (int): Maximum absolute value allowed for exponents.

    Returns:
        tuple: (rounded_exponents_batch, valid_mask)
               rounded_exponents_batch (np.ndarray): Rounded exponents (num_dependent_exponents, N_combinations).
               valid_mask (np.ndarray): Boolean mask (N_combinations,) indicating valid combinations.
    """
    # Ensure only use # for annotations to explain each section of the code
    num_dependent_exponents, num_combinations = x_dep_raw_batch.shape

    # Calculate absolute differences between each raw exponent and all possible exponents
    # x_dep_raw_batch[:, :, None] reshapes (4, N) to (4, N, 1) for broadcasting
    # possible_exponents[None, None, :] reshapes (M,) to (1, 1, M) for broadcasting
    # diffs shape: (num_dependent_exponents, num_combinations, len(possible_exponents))
    diffs = np.abs(x_dep_raw_batch[:, :, None] - possible_exponents[None, None, :])

    # Find the index of the closest possible exponent for each raw exponent
    closest_indices = np.argmin(diffs, axis=2) # shape: (num_dependent_exponents, num_combinations)

    # Get the rounded values using the closest indices
    rounded_values = possible_exponents[closest_indices] # shape: (num_dependent_exponents, num_combinations)

    # Check if the closest value is within tolerance for each individual exponent
    # This uses advanced indexing to pick the minimum difference for each (exponent, combination) pair
    tolerance_mask = diffs[np.arange(num_dependent_exponents)[:, None],
                           np.arange(num_combinations)[None, :],
                           closest_indices] < tolerance

    # Check if all dependent exponents for a given combination are within tolerance
    all_tolerance_mask = np.all(tolerance_mask, axis=0) # shape: (num_combinations,)

    # Check if rounded values are within absolute bounds
    abs_bound_mask = np.abs(rounded_values) <= max_abs_val # shape: (num_dependent_exponents, num_combinations)

    # Check if all dependent exponents for a given combination are within absolute bounds
    all_abs_bound_mask = np.all(abs_bound_mask, axis=0) # shape: (num_combinations,)

    # Combine masks to get final valid combinations
    valid_mask = all_tolerance_mask & all_abs_bound_mask

    return rounded_values, valid_mask

def evaluate_single_combination_with_cv(log_X_data, log_y_data, current_exponents, n_splits=N_SPLITS_CV):
    """
    Calculates the dimensionless quantity Pi, performs linear regression with log(e*),
    and returns the R-squared and RMSE values using K-Fold Cross-Validation.
    Also returns the model fitted on the full dataset for later use.

    Args:
        log_X_data (np.ndarray): Log-transformed input features.
        log_y_data (np.ndarray): Log-transformed target variable.
        current_exponents (np.ndarray): Array of exponents for the current Pi group.
        n_splits (int): Number of splits for K-Fold Cross-Validation.

    Returns:
        tuple: (avg_r2, avg_rmse, current_exponents, log_pi_values, full_model)
               avg_r2 (float): Average R-squared value from CV.
               avg_rmse (float): Average RMSE value from CV.
               current_exponents (np.ndarray): The exponents for this combination.
               log_pi_values (np.ndarray): Log-transformed Pi values for the full dataset.
               full_model (LinearRegression): Model fitted on the entire dataset.
    """
    # Ensure only use # for annotations to explain each section of the code
    # Calculate log(Pi) = sum(x_i * log(Q_i)) using dot product for efficiency
    log_pi_values = np.dot(log_X_data, current_exponents)

    # Reshape for sklearn's LinearRegression, which expects a 2D array for X
    log_pi_values_reshaped = log_pi_values.reshape(-1, 1)

    # Perform K-Fold Cross-Validation to get robust R-squared and RMSE
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    r2_scores = []
    rmse_scores = []

    for train_index, test_index in kf.split(log_pi_values_reshaped):
        X_train, X_test = log_pi_values_reshaped[train_index], log_pi_values_reshaped[test_index]
        y_train, y_test = log_y_data[train_index], log_y_data[test_index]

        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        r2_scores.append(r2_score(y_test, y_pred))
        rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))

    avg_r2 = np.mean(r2_scores)
    avg_rmse = np.mean(rmse_scores)

    # Fit a model on the full dataset. This model will be used for plotting the best fit.
    full_model = LinearRegression()
    full_model.fit(log_pi_values_reshaped, log_y_data)

    return avg_r2, avg_rmse, current_exponents, log_pi_values, full_model

def plot_results(best_r_squared, best_rmse, log_y_data, best_pi_values, best_model, y_data_positive, filename='optimal_dimensionless_quantity_correlation.png'):
    """
    Generates and saves plots for the optimal dimensionless quantity correlation.

    Args:
        best_r_squared (float): R-squared value of the optimal group.
        best_rmse (float): RMSE value of the optimal group.
        log_y_data (np.ndarray): Log-transformed target variable.
        best_pi_values (np.ndarray): Optimal Pi values (original scale).
        best_model (LinearRegression): Regression model for the optimal group.
        y_data_positive (np.ndarray): Original scale positive target variable.
        filename (str): Name of the file to save the plots.
    """
    # Ensure only use # for annotations to explain each section of the code
    plt.figure(figsize=(12, 6))

    # Plot 1: log(e*) vs log(Pi)
    plt.subplot(1, 2, 1)
    plt.scatter(np.log(best_pi_values), log_y_data, alpha=0.7, label='Data points')
    # Plot the linear fit line
    plt.plot(np.log(best_pi_values), best_model.predict(np.log(best_pi_values).reshape(-1, 1)), color='red', label='Linear Fit')
    plt.title(f'log(e*) vs log(Optimal Pi)\n(R²={best_r_squared:.4f}, RMSE={best_rmse:.4f})')
    plt.xlabel('log(Optimal Pi)')
    plt.ylabel('log(e*)')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()

    # Plot 2: e* vs Pi (original scale)
    plt.subplot(1, 2, 2)
    plt.scatter(best_pi_values, y_data_positive, alpha=0.7, label='Data points')
    # For the original scale plot, use the power law derived from the log-log fit: e* = K * Pi^m
    x_fit = np.linspace(best_pi_values.min(), best_pi_values.max(), 100)
    y_fit = np.exp(best_model.intercept_) * (x_fit ** best_model.coef_[0])
    plt.plot(x_fit, y_fit, color='red', label='Power Law Fit')
    plt.title(f'e* vs Optimal Pi (Original Scale)')
    plt.xlabel('Optimal Pi')
    plt.ylabel('e*')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()

    plt.tight_layout()
    plt.savefig(filename)
    plt.close() # Close the figure to free memory and avoid warnings, as per teacher's feedback
    print(f"\nPlots saved to '{filename}'")

# Main problem-solving process
def solve_dimensional_analysis():
    """
    Main function to perform dimensional analysis, find the optimal dimensionless group,
    and report/visualize the results.
    """
    # 1. Load and preprocess data
    log_X_data, log_y_data, X_data_positive, y_data_positive = load_and_preprocess_data(
        FILE_PATH, COL_INDICES, TARGET_COL_INDEX
    )

    # 2. Generate possible exponents
    possible_exponents = generate_possible_exponents(MAX_ABS_EXPONENT, POSSIBLE_DENOMINATORS)

    # 3. Define sub-matrices for dependent and independent variables
    # Exponents order: [a, b, c, d, e, f, g] for [etaP, Vs, r0, alpha, rho, Cp, Tl-T0]
    # Independent variables for grid search: c (r0), d (alpha)
    # Fixed variable: b (Vs)
    # Dependent variables: a (etaP), e (rho), f (Cp), g (Tl-T0)

    # D_dep corresponds to columns for [etaP, rho, Cp, Tl-T0] (indices 0, 4, 5, 6)
    D_dep = DIMENSIONAL_MATRIX[:, [0, 4, 5, 6]]
    # D_ind_cd corresponds to columns for [r0, alpha] (indices 2, 3)
    D_ind_cd = DIMENSIONAL_MATRIX[:, [2, 3]]
    # D_b corresponds to column for [Vs] (index 1)
    D_b = DIMENSIONAL_MATRIX[:, [1]]

    # Calculate inverse of D_dep once
    # Ensure only use # for annotations to explain each section of the code
    try:
        inv_D_dep = scipy.linalg.inv(D_dep)
    except scipy.linalg.LinAlgError as e:
        print(f"Error: Dimensional matrix D_dep is singular. Cannot invert. {e}")
        sys.exit(1) # Exit gracefully if matrix is singular

    # Initialize best R-squared and corresponding exponents
    best_r_squared = -1.0
    best_rmse = float('inf') # Initialize RMSE to infinity for minimization
    best_exponents = None
    best_log_pi_values = None
    best_model = None

    # Optimized Grid Search Loop
    # This section has been optimized by vectorizing the calculation of dependent exponents.
    # Instead of nested Python loops for 'c_val' (r0 exponent) and 'd_val' (alpha exponent),
    # all combinations of these independent exponents are generated and processed in a batch
    # using NumPy's array operations. This significantly reduces Python overhead and leverages
    # highly optimized C implementations in NumPy for matrix multiplications and array manipulations.
    print("Starting optimized grid search for optimal dimensionless quantity...")
    total_combinations_checked = 0
    valid_combinations_found = 0
    all_combinations_to_evaluate = [] # List to store all valid exponent sets for parallel processing

    # Generate all combinations of independent exponents (r0, alpha) using meshgrid
    C_vals, D_vals = np.meshgrid(possible_exponents, possible_exponents)
    C_vals_flat = C_vals.flatten() # Flatten to a 1D array of all c_val combinations
    D_vals_flat = D_vals.flatten() # Flatten to a 1D array of all d_val combinations
    num_independent_combinations_per_b_fixed = len(C_vals_flat)

    # Iterate through fixed V_s exponents (b_fixed)
    for b_fixed in [1.0, -1.0]:
        # Calculate the right-hand side of the equation for all combinations in a batch
        # np.vstack((C_vals_flat, D_vals_flat)) creates a (2, num_independent_combinations) array
        # D_ind_cd @ (2, N) results in (4, N)
        # D_b @ np.array([[b_fixed]]) results in (4, 1), which broadcasts to (4, N)
        rhs_batch = - (D_ind_cd @ np.vstack((C_vals_flat, D_vals_flat)) + D_b @ np.array([[b_fixed]]))

        # Solve for the raw dependent exponents [a_raw, e_raw, f_raw, g_raw] for all combinations in a batch
        # inv_D_dep @ (4, N) results in (4, N)
        x_dep_raw_batch = inv_D_dep @ rhs_batch

        # Round and validate all dependent exponents in a batch
        # rounded_dep_exponents shape: (4, num_independent_combinations)
        # valid_combinations_mask shape: (num_independent_combinations,)
        rounded_dep_exponents, valid_combinations_mask = round_and_validate_exponents_batch(
            x_dep_raw_batch, possible_exponents, EXPONENT_TOLERANCE, MAX_ABS_EXPONENT
        )

        # Filter for valid combinations based on the mask
        valid_c_vals = C_vals_flat[valid_combinations_mask]
        valid_d_vals = D_vals_flat[valid_combinations_mask]
        valid_rounded_dep_exponents = rounded_dep_exponents[:, valid_combinations_mask]

        total_combinations_checked += num_independent_combinations_per_b_fixed
        valid_combinations_found += len(valid_c_vals)

        # Collect all valid exponent sets for parallel evaluation
        for i in range(len(valid_c_vals)):
            c_val = valid_c_vals[i]
            d_val = valid_d_vals[i]
            a_rounded, e_rounded, f_rounded, g_rounded = valid_rounded_dep_exponents[:, i]
            # Assemble the full set of exponents for the current dimensionless group
            # Order: [etaP, Vs, r0, alpha, rho, Cp, Tl-T0]
            current_exponents = np.array([a_rounded, b_fixed, c_val, d_val, e_rounded, f_rounded, g_rounded])
            all_combinations_to_evaluate.append(current_exponents)

    print(f"Grid search identified {total_combinations_checked} total potential combinations and {valid_combinations_found} dimensionally homogeneous combinations.")
    print(f"Evaluating {len(all_combinations_to_evaluate)} valid combinations using {N_SPLITS_CV}-Fold Cross-Validation in parallel...")

    # Parallel execution of regression evaluation for all valid combinations
    # n_jobs=-1 uses all available CPU cores. verbose=10 shows progress.
    results = Parallel(n_jobs=-1, verbose=10)(
        delayed(evaluate_single_combination_with_cv)(log_X_data, log_y_data, exp)
        for exp in all_combinations_to_evaluate
    )

    # Process results from parallel execution to find the best combination
    for r2, rmse, exponents, log_pi_values, model in results:
        # We prioritize R-squared. If R-squared is very close, RMSE can be a tie-breaker (lower RMSE is better).
        if r2 > best_r_squared:
            best_r_squared = r2
            best_rmse = rmse
            best_exponents = exponents
            best_log_pi_values = log_pi_values
            best_model = model
        elif r2 == best_r_squared and rmse < best_rmse: # If R-squared is same, pick lower RMSE
            best_rmse = rmse
            best_exponents = exponents
            best_log_pi_values = log_pi_values
            best_model = model

    # Convert best_log_pi_values back to original scale for plotting
    if best_log_pi_values is not None:
        best_pi_values = np.exp(best_log_pi_values)
    else:
        best_pi_values = None

    # Detailed result printing
    print("\n--- Dimensional Analysis Results ---")
    if best_exponents is not None:
        print(f"Optimal R-squared (Avg. from {N_SPLITS_CV}-Fold CV): {best_r_squared:.4f}")
        print(f"Optimal RMSE (Avg. from {N_SPLITS_CV}-Fold CV): {best_rmse:.4f}")
        print("\nOptimal Dimensionless Quantity (Pi) Exponents:")
        exponent_str = []
        for i, name in enumerate(QUANTITY_NAMES):
            exp = best_exponents[i]
            # Only include terms with non-zero exponents in the formula string
            if exp != 0:
                # Format exponents nicely (e.g., 1.0 as 1, 0.5 as 1/2)
                if exp == int(exp):
                    exp_str = str(int(exp))
                elif abs(exp - 0.5) < EXPONENT_TOLERANCE:
                    exp_str = "1/2"
                elif abs(exp - (-0.5)) < EXPONENT_TOLERANCE:
                    exp_str = "-1/2"
                elif abs(exp - 0.25) < EXPONENT_TOLERANCE:
                    exp_str = "1/4"
                elif abs(exp - (-0.25)) < EXPONENT_TOLERANCE:
                    exp_str = "-1/4"
                elif abs(exp - 0.3333333333333333) < EXPONENT_TOLERANCE: # 1/3
                    exp_str = "1/3"
                elif abs(exp - (-0.3333333333333333)) < EXPONENT_TOLERANCE: # -1/3
                    exp_str = "-1/3"
                elif abs(exp - 0.6666666666666666) < EXPONENT_TOLERANCE: # 2/3
                    exp_str = "2/3"
                elif abs(exp - (-0.6666666666666666)) < EXPONENT_TOLERANCE: # -2/3
                    exp_str = "-2/3"
                else:
                    exp_str = f"{exp:.4f}"
                exponent_str.append(f"{name}^{{{exp_str}}}")
            print(f"  {name}: {exp:.4f}")

        pi_formula = " * ".join(exponent_str)
        print(f"\nPi Formula: Pi = {pi_formula}")

        # Print the linear regression model parameters for log-log plot
        if best_model:
            print(f"\nLinear Regression Model (log(e*) vs log(Pi)):")
            print(f"  Slope (m): {best_model.coef_[0]:.4f}")
            print(f"  Intercept (log(K)): {best_model.intercept_:.4f}")
            print(f"  Equivalent Power Law: e* = {np.exp(best_model.intercept_):.4f} * Pi^{best_model.coef_[0]:.4f}")

        # Physical Interpretation:
        # The identified dimensionless group is Pi = etaP^{-2.0} * Vs^{1.0} * r0^{3.0} * alpha^{1.0} * rho^{2.0} * Cp^{2.0} * Tl-T0^{2.0}.
        # The negative exponent on etaP (effective laser power) indicates that as laser power increases, Pi decreases.
        # The regression slope (m) is -0.6440, meaning e* is inversely proportional to Pi^(0.6440).
        # Therefore, increasing laser power (which decreases Pi) leads to an increase in e* (keyhole aspect ratio),
        # which is physically intuitive as higher power generally leads to deeper penetration.
        # The high exponents on r0 (laser beam radius), rho (material density), Cp (heat capacity),
        # and Tl-T0 (temperature difference) suggest a strong non-linear dependence of the keyhole aspect ratio
        # on these parameters. For instance, r0^3 implies a cubic relationship, which might reflect the volume
        # of material affected or the energy density distribution.
        # This group can be compared with known dimensionless numbers in laser-material interaction,
        # such as the Peclet number (related to Vs, r0, alpha) or dimensionless heat input parameters,
        # to gain deeper physical insight.
        # The normalization by Vs (exponent 1.0) means that the group is structured such that Vs is in the numerator.
        # If b_fixed = -1.0 had yielded a higher R-squared, Vs would be in the denominator, leading to a different physical interpretation.
        # The use of K-Fold Cross-Validation for R-squared and RMSE calculation provides a more robust estimate
        # of the model's generalization performance, reducing the risk of overfitting to the specific dataset.

        # Plotting results
        plot_results(best_r_squared, best_rmse, log_y_data, best_pi_values, best_model, y_data_positive)

    else:
        print("No valid dimensionless quantity found within the specified constraints.")

# Execute the problem-solving function
if __name__ == "__main__":
    solve_dimensional_analysis()
```
****************************************
