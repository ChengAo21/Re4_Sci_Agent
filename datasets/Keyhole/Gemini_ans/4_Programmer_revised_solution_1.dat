
****************************************
```python
# Imports
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import scipy.linalg
import sys # For graceful exit

# Technical explanation for the solving algorithm
# The problem aims to identify an optimal dimensionless quantity (Pi) formed by combining seven physical parameters,
# such that Pi exhibits the highest coefficient of determination (R-squared) with the keyhole aspect ratio (e*).
# The exponents of the physical quantities in Pi must be integers or rational fractions, with absolute values not exceeding 3,
# and the resulting dimensionless exponents must be normalized by the laser scan speed (V_s).

# Algorithm: Systematic Dimensional Analysis with Fixed V_s Exponent and Optimized Grid Search
# 1.  **Dimensional Analysis Setup**:
#     -   Define the fundamental dimensions (Mass [M], Length [L], Time [T], Temperature [Theta]) for each of the seven physical quantities:
#         eta P (Power): [M L^2 T^-3]
#         V_s (Speed): [L T^-1]
#         r_0 (Length): [L]
#         alpha (Thermal Diffusivity): [L^2 T^-1]
#         rho (Density): [M L^-3]
#         C_p (Heat Capacity): [L^2 T^-2 Theta^-1]
#         T_l-T_0 (Temperature Difference): [Theta]
#     -   Construct a dimensional matrix (D) where rows represent fundamental dimensions and columns represent physical quantities.
#         For a dimensionless quantity Pi = Q1^x1 * Q2^x2 * ... * Qn^xn, the sum of (exponent * dimension_vector) for each fundamental dimension must be zero.
#         This translates to D @ x = 0, where x is the vector of exponents.
# 2.  **Exponent Constraints and Normalization**:
#     -   The problem requires exponents to be integers or rational fractions within [-3, 3]. A predefined set of rational numbers (e.g., integers and half-integers, or quarters) is generated for the grid search.
#     -   "Normalized by V_s" implies that the exponent of V_s (let's call it 'b') should be a fixed reference, typically 1 or -1. This significantly reduces the search space.
# 3.  **Reduced Search Space via Dimensional Homogeneity**:
#     -   The dimensional matrix D (4x7) implies that 4 exponents can be expressed as linear combinations of the remaining 3.
#     -   By fixing the exponent of V_s (b) to either 1 or -1, we are left with 2 independent exponents to iterate over (e.g., r_0 and alpha). The remaining 4 exponents (eta P, rho, C_p, T_l-T_0) can then be determined by solving the system of linear equations derived from D @ x = 0.
#     -   Specifically, we partition the exponent vector `x` into `x_independent` (e.g., `r_0`, `alpha`) and `x_dependent` (e.g., `eta P`, `rho`, `C_p`, `T_l-T_0`), with `V_s`'s exponent `b` being fixed. The equation becomes `D_dep @ x_dep = -D_ind @ x_ind - D_b * b_fixed`.
# 4.  **Optimized Grid Search and R-squared Evaluation**:
#     -   Instead of nested Python loops for independent exponents, all combinations of independent exponents are generated using `numpy.meshgrid`.
#     -   The calculation of dependent exponents is vectorized using NumPy's array operations, significantly speeding up this part of the search.
#     -   The rounding and validation of dependent exponents are also performed in a vectorized manner.
#     -   For each *valid* combination of exponents, the dimensionless quantity Pi is constructed for all data points: `Pi = (eta P)^a * (V_s)^b * ...`.
#     -   To evaluate the correlation, a linear regression is performed between `log(e*)` and `log(Pi)`. This is because a power-law relationship `e* = K * Pi^m` transforms into a linear relationship `log(e*) = log(K) + m * log(Pi)`.
#     -   The R-squared value from this linear regression is calculated.
# 5.  **Optimal Group Identification**:
#     -   The set of exponents that yields the highest R-squared value is identified as the optimal dimensionless quantity.
# 6.  **Result Reporting and Visualization**:
#     -   The optimal R-squared, the formula for the optimal dimensionless quantity, and the corresponding exponents are printed.
#     -   Plots of `log(e*)` vs `log(Pi_optimal)` and `e*` vs `Pi_optimal` are generated and saved to visualize the correlation.
#     -   `plt.close()` is added after saving figures to prevent memory warnings.

# Constants and configurations
FILE_PATH = 'Q:/Work2/LLM_/test_Agent/dataset_keyhole.csv'
# Column indices (0-indexed) for the physical quantities
# eta P (col 3), V_s (col 4), r_0 (col 5), alpha (col 6), rho (col 7), C_p (col 8), T_l-T_0 (col 11)
# e* (last column)
COL_INDICES = [2, 3, 4, 5, 6, 7, 10] # Corresponding to etaP, Vs, r0, alpha, rho, Cp, Tl-T0
QUANTITY_NAMES = ['etaP', 'Vs', 'r0', 'alpha', 'rho', 'Cp', 'Tl-T0']
TARGET_COL_INDEX = -1 # e*

# Define the dimensional matrix D for [M, L, T, Theta]
# Quantities order: [etaP, Vs, r0, alpha, rho, Cp, Tl-T0]
# Dimensions:
# M: Mass
# L: Length
# T: Time
# Theta: Temperature
DIMENSIONAL_MATRIX = np.array([
    # etaP Vs r0 alpha rho Cp Tl-T0
    [1, 0, 0, 0, 1, 0, 0],  # M
    [2, 1, 1, 2, -3, 2, 0], # L
    [-3, -1, 0, -1, 0, -2, 0], # T
    [0, 0, 0, 0, 0, -1, 1]  # Theta
])

# Tolerance for floating point comparisons when rounding exponents
EXPONENT_TOLERANCE = 1e-5
MAX_ABS_EXPONENT = 3 # Maximum absolute value allowed for exponents

# Helper functions for modularity and clarity

def load_and_preprocess_data(file_path, col_indices, target_col_index):
    """
    Loads data from CSV, extracts relevant columns, and preprocesses for log-log regression.
    Filters out rows with non-positive values.

    Args:
        file_path (str): Path to the CSV data file.
        col_indices (list): List of 0-indexed column indices for physical quantities.
        target_col_index (int): 0-indexed column index for the target variable (e*).

    Returns:
        tuple: (log_X_data, log_y_data, X_data_positive, y_data_positive)
               log-transformed input features, log-transformed target, original positive features, original positive target.
    """
    # Ensure only use # for annotations to explain each section of the code
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}. Please check the path.")
        sys.exit(1) # Exit gracefully if file is not found
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        sys.exit(1) # Exit gracefully for other file reading errors

    # Extract relevant columns
    X_data = df.iloc[:, col_indices].values
    y_data = df.iloc[:, target_col_index].values

    # Handle potential zero or negative values by taking log
    # Filter out rows where any value is non-positive before log transformation.
    initial_rows = X_data.shape[0]
    positive_mask = (X_data > 0).all(axis=1) & (y_data > 0)
    X_data_positive = X_data[positive_mask]
    y_data_positive = y_data[positive_mask]

    if X_data_positive.shape[0] == 0:
        print("Error: No positive data points found for log transformation. Exiting.")
        sys.exit(1) # Exit if no valid data points remain
    elif X_data_positive.shape[0] < initial_rows:
        # Warning message for discarded data points
        print(f"Warning: {initial_rows - X_data_positive.shape[0]} rows discarded due to non-positive values.")

    log_X_data = np.log(X_data_positive)
    log_y_data = np.log(y_data_positive)

    return log_X_data, log_y_data, X_data_positive, y_data_positive

def generate_possible_exponents(max_abs_val=3, denominators=[1, 2, 3, 4]):
    """
    Generates a sorted array of possible rational exponents within a given range.

    Args:
        max_abs_val (int): Maximum absolute value for exponents.
        denominators (list): List of denominators to consider for rational fractions.

    Returns:
        np.ndarray: Sorted array of unique possible exponents.
    """
    # Ensure only use # for annotations to explain each section of the code
    # Generate raw possible exponents (e.g., k/d)
    possible_exponents_raw = np.array([k/d for d in denominators for k in range(-max_abs_val * max(denominators), max_abs_val * max(denominators) + 1)])
    # Filter to ensure absolute value is within max_abs_val and get unique values
    possible_exponents = np.unique(possible_exponents_raw[np.abs(possible_exponents_raw) <= max_abs_val])
    possible_exponents.sort() # Sort for consistent iteration
    return possible_exponents

def round_and_validate_exponents_batch(x_dep_raw_batch, possible_exponents, tolerance, max_abs_val):
    """
    Rounds a batch of raw dependent exponents to the nearest rational values
    and validates them against tolerance and absolute bounds.

    Args:
        x_dep_raw_batch (np.ndarray): Raw dependent exponents (num_dependent_exponents, N_combinations).
        possible_exponents (np.ndarray): Array of allowed rational exponents.
        tolerance (float): Tolerance for rounding.
        max_abs_val (int): Maximum absolute value allowed for exponents.

    Returns:
        tuple: (rounded_exponents_batch, valid_mask)
               rounded_exponents_batch (np.ndarray): Rounded exponents (num_dependent_exponents, N_combinations).
               valid_mask (np.ndarray): Boolean mask (N_combinations,) indicating valid combinations.
    """
    # Ensure only use # for annotations to explain each section of the code
    num_dependent_exponents, num_combinations = x_dep_raw_batch.shape

    # Calculate absolute differences between each raw exponent and all possible exponents
    # x_dep_raw_batch[:, :, None] reshapes (4, N) to (4, N, 1) for broadcasting
    # possible_exponents[None, None, :] reshapes (M,) to (1, 1, M) for broadcasting
    # diffs shape: (num_dependent_exponents, num_combinations, len(possible_exponents))
    diffs = np.abs(x_dep_raw_batch[:, :, None] - possible_exponents[None, None, :])

    # Find the index of the closest possible exponent for each raw exponent
    closest_indices = np.argmin(diffs, axis=2) # shape: (num_dependent_exponents, num_combinations)

    # Get the rounded values using the closest indices
    rounded_values = possible_exponents[closest_indices] # shape: (num_dependent_exponents, num_combinations)

    # Check if the closest value is within tolerance for each individual exponent
    # This uses advanced indexing to pick the minimum difference for each (exponent, combination) pair
    tolerance_mask = diffs[np.arange(num_dependent_exponents)[:, None],
                           np.arange(num_combinations)[None, :],
                           closest_indices] < tolerance

    # Check if all dependent exponents for a given combination are within tolerance
    all_tolerance_mask = np.all(tolerance_mask, axis=0) # shape: (num_combinations,)

    # Check if rounded values are within absolute bounds
    abs_bound_mask = np.abs(rounded_values) <= max_abs_val # shape: (num_dependent_exponents, num_combinations)

    # Check if all dependent exponents for a given combination are within absolute bounds
    all_abs_bound_mask = np.all(abs_bound_mask, axis=0) # shape: (num_combinations,)

    # Combine masks to get final valid combinations
    valid_mask = all_tolerance_mask & all_abs_bound_mask

    return rounded_values, valid_mask

def evaluate_dimensionless_group(log_X_data, current_exponents, log_y_data):
    """
    Calculates the dimensionless quantity Pi, performs linear regression with log(e*),
    and returns the R-squared value, the regression model, and Pi values.

    Args:
        log_X_data (np.ndarray): Log-transformed input features.
        current_exponents (np.ndarray): Array of exponents for the current Pi group.
        log_y_data (np.ndarray): Log-transformed target variable.

    Returns:
        tuple: (r_squared, model, pi_values)
               r_squared (float): R-squared value.
               model (LinearRegression): Fitted regression model.
               pi_values (np.ndarray): Original scale Pi values.
    """
    # Ensure only use # for annotations to explain each section of the code
    # Calculate log(Pi) = sum(x_i * log(Q_i)) using dot product for efficiency
    log_pi_values = np.dot(log_X_data, current_exponents)

    # Reshape for sklearn's LinearRegression, which expects a 2D array for X
    log_pi_values_reshaped = log_pi_values.reshape(-1, 1)

    # Perform linear regression: log(e*) = m * log(Pi) + c
    model = LinearRegression()
    model.fit(log_pi_values_reshaped, log_y_data)
    y_pred_log = model.predict(log_pi_values_reshaped)
    r_squared = r2_score(log_y_data, y_pred_log)

    # Convert log(Pi) back to Pi values for original scale plotting
    pi_values = np.exp(log_pi_values)

    return r_squared, model, pi_values

def plot_results(best_r_squared, log_y_data, best_pi_values, best_model, y_data_positive, filename='optimal_dimensionless_quantity_correlation.png'):
    """
    Generates and saves plots for the optimal dimensionless quantity correlation.

    Args:
        best_r_squared (float): R-squared value of the optimal group.
        log_y_data (np.ndarray): Log-transformed target variable.
        best_pi_values (np.ndarray): Optimal Pi values (original scale).
        best_model (LinearRegression): Regression model for the optimal group.
        y_data_positive (np.ndarray): Original scale positive target variable.
        filename (str): Name of the file to save the plots.
    """
    # Ensure only use # for annotations to explain each section of the code
    plt.figure(figsize=(12, 6))

    # Plot 1: log(e*) vs log(Pi)
    plt.subplot(1, 2, 1)
    plt.scatter(np.log(best_pi_values), log_y_data, alpha=0.7, label='Data points')
    # Plot the linear fit line
    plt.plot(np.log(best_pi_values), best_model.predict(np.log(best_pi_values).reshape(-1, 1)), color='red', label='Linear Fit')
    plt.title(f'log(e*) vs log(Optimal Pi) (R²={best_r_squared:.4f})')
    plt.xlabel('log(Optimal Pi)')
    plt.ylabel('log(e*)')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()

    # Plot 2: e* vs Pi (original scale)
    plt.subplot(1, 2, 2)
    plt.scatter(best_pi_values, y_data_positive, alpha=0.7, label='Data points')
    # For the original scale plot, use the power law derived from the log-log fit: e* = K * Pi^m
    x_fit = np.linspace(best_pi_values.min(), best_pi_values.max(), 100)
    y_fit = np.exp(best_model.intercept_) * (x_fit ** best_model.coef_[0])
    plt.plot(x_fit, y_fit, color='red', label='Power Law Fit')
    plt.title(f'e* vs Optimal Pi (Original Scale)')
    plt.xlabel('Optimal Pi')
    plt.ylabel('e*')
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.legend()

    plt.tight_layout()
    plt.savefig(filename)
    plt.close() # Close the figure to free memory and avoid warnings, as per teacher's feedback
    print(f"\nPlots saved to '{filename}'")

# Main problem-solving process
def solve_dimensional_analysis():
    """
    Main function to perform dimensional analysis, find the optimal dimensionless group,
    and report/visualize the results.
    """
    # 1. Load and preprocess data
    log_X_data, log_y_data, X_data_positive, y_data_positive = load_and_preprocess_data(
        FILE_PATH, COL_INDICES, TARGET_COL_INDEX
    )

    # 2. Generate possible exponents
    possible_exponents = generate_possible_exponents(MAX_ABS_EXPONENT)

    # 3. Define sub-matrices for dependent and independent variables
    # Exponents order: [a, b, c, d, e, f, g] for [etaP, Vs, r0, alpha, rho, Cp, Tl-T0]
    # Independent variables for grid search: c (r0), d (alpha)
    # Fixed variable: b (Vs)
    # Dependent variables: a (etaP), e (rho), f (Cp), g (Tl-T0)

    # D_dep corresponds to columns for [etaP, rho, Cp, Tl-T0] (indices 0, 4, 5, 6)
    D_dep = DIMENSIONAL_MATRIX[:, [0, 4, 5, 6]]
    # D_ind_cd corresponds to columns for [r0, alpha] (indices 2, 3)
    D_ind_cd = DIMENSIONAL_MATRIX[:, [2, 3]]
    # D_b corresponds to column for [Vs] (index 1)
    D_b = DIMENSIONAL_MATRIX[:, [1]]

    # Calculate inverse of D_dep once
    # Ensure only use # for annotations to explain each section of the code
    try:
        inv_D_dep = scipy.linalg.inv(D_dep)
    except scipy.linalg.LinAlgError as e:
        print(f"Error: Dimensional matrix D_dep is singular. Cannot invert. {e}")
        sys.exit(1) # Exit gracefully if matrix is singular

    # Initialize best R-squared and corresponding exponents
    best_r_squared = -1.0
    best_exponents = None
    best_pi_values = None
    best_model = None

    # Optimized Grid Search Loop
    # This section has been optimized by vectorizing the calculation of dependent exponents.
    # Instead of nested Python loops for 'c_val' (r0 exponent) and 'd_val' (alpha exponent),
    # all combinations of these independent exponents are generated and processed in a batch
    # using NumPy's array operations. This significantly reduces Python overhead and leverages
    # highly optimized C implementations in NumPy for matrix multiplications and array manipulations.
    print("Starting optimized grid search for optimal dimensionless quantity...")
    total_combinations_checked = 0
    valid_combinations_found = 0

    # Generate all combinations of independent exponents (r0, alpha) using meshgrid
    C_vals, D_vals = np.meshgrid(possible_exponents, possible_exponents)
    C_vals_flat = C_vals.flatten() # Flatten to a 1D array of all c_val combinations
    D_vals_flat = D_vals.flatten() # Flatten to a 1D array of all d_val combinations
    num_independent_combinations = len(C_vals_flat)

    # Iterate through fixed V_s exponents (b_fixed)
    for b_fixed in [1.0, -1.0]:
        # Calculate the right-hand side of the equation for all combinations in a batch
        # np.vstack((C_vals_flat, D_vals_flat)) creates a (2, num_independent_combinations) array
        # D_ind_cd @ (2, N) results in (4, N)
        # D_b @ np.array([[b_fixed]]) results in (4, 1), which broadcasts to (4, N)
        rhs_batch = - (D_ind_cd @ np.vstack((C_vals_flat, D_vals_flat)) + D_b @ np.array([[b_fixed]]))

        # Solve for the raw dependent exponents [a_raw, e_raw, f_raw, g_raw] for all combinations in a batch
        # inv_D_dep @ (4, N) results in (4, N)
        x_dep_raw_batch = inv_D_dep @ rhs_batch

        # Round and validate all dependent exponents in a batch
        # rounded_dep_exponents shape: (4, num_independent_combinations)
        # valid_combinations_mask shape: (num_independent_combinations,)
        rounded_dep_exponents, valid_combinations_mask = round_and_validate_exponents_batch(
            x_dep_raw_batch, possible_exponents, EXPONENT_TOLERANCE, MAX_ABS_EXPONENT
        )

        # Filter for valid combinations based on the mask
        valid_c_vals = C_vals_flat[valid_combinations_mask]
        valid_d_vals = D_vals_flat[valid_combinations_mask]
        valid_rounded_dep_exponents = rounded_dep_exponents[:, valid_combinations_mask]

        total_combinations_checked += num_independent_combinations
        valid_combinations_found += len(valid_c_vals)

        # Iterate only through valid combinations to perform regression
        # This loop is necessary because sklearn's LinearRegression does not easily vectorize
        # across multiple sets of Pi values. However, the number of valid combinations
        # is typically much smaller than the total search space, making this part efficient
        # after the initial batch calculations.
        for i in range(len(valid_c_vals)):
            c_val = valid_c_vals[i]
            d_val = valid_d_vals[i]
            a_rounded, e_rounded, f_rounded, g_rounded = valid_rounded_dep_exponents[:, i]

            # Assemble the full set of exponents for the current dimensionless group
            # Order: [etaP, Vs, r0, alpha, rho, Cp, Tl-T0]
            current_exponents = np.array([a_rounded, b_fixed, c_val, d_val, e_rounded, f_rounded, g_rounded])

            # Evaluate the dimensionless group and perform regression
            current_r_squared, model, pi_values = evaluate_dimensionless_group(
                log_X_data, current_exponents, log_y_data
            )

            # Update best R-squared if current one is better
            if current_r_squared > best_r_squared:
                best_r_squared = current_r_squared
                best_exponents = current_exponents
                best_pi_values = pi_values
                best_model = model

    print(f"Grid search completed. Total combinations checked: {total_combinations_checked}. Valid combinations found: {valid_combinations_found}.")

    # Detailed result printing
    print("\n--- Dimensional Analysis Results ---")
    if best_exponents is not None:
        print(f"Optimal R-squared: {best_r_squared:.4f}")
        print("\nOptimal Dimensionless Quantity (Pi) Exponents:")
        exponent_str = []
        for i, name in enumerate(QUANTITY_NAMES):
            exp = best_exponents[i]
            # Only include terms with non-zero exponents in the formula string
            if exp != 0:
                # Format exponents nicely (e.g., 1.0 as 1, 0.5 as 1/2)
                if exp == int(exp):
                    exp_str = str(int(exp))
                elif abs(exp - 0.5) < EXPONENT_TOLERANCE:
                    exp_str = "1/2"
                elif abs(exp - (-0.5)) < EXPONENT_TOLERANCE:
                    exp_str = "-1/2"
                elif abs(exp - 0.25) < EXPONENT_TOLERANCE:
                    exp_str = "1/4"
                elif abs(exp - (-0.25)) < EXPONENT_TOLERANCE:
                    exp_str = "-1/4"
                elif abs(exp - 0.3333333333333333) < EXPONENT_TOLERANCE: # 1/3
                    exp_str = "1/3"
                elif abs(exp - (-0.3333333333333333)) < EXPONENT_TOLERANCE: # -1/3
                    exp_str = "-1/3"
                elif abs(exp - 0.6666666666666666) < EXPONENT_TOLERANCE: # 2/3
                    exp_str = "2/3"
                elif abs(exp - (-0.6666666666666666)) < EXPONENT_TOLERANCE: # -2/3
                    exp_str = "-2/3"
                else:
                    exp_str = f"{exp:.4f}"
                exponent_str.append(f"{name}^{{{exp_str}}}")
            print(f"  {name}: {exp:.4f}")

        pi_formula = " * ".join(exponent_str)
        print(f"\nPi Formula: Pi = {pi_formula}")

        # Print the linear regression model parameters for log-log plot
        if best_model:
            print(f"\nLinear Regression Model (log(e*) vs log(Pi)):")
            print(f"  Slope (m): {best_model.coef_[0]:.4f}")
            print(f"  Intercept (log(K)): {best_model.intercept_:.4f}")
            print(f"  Equivalent Power Law: e* = {np.exp(best_model.intercept_):.4f} * Pi^{best_model.coef_[0]:.4f}")

        # Physical Interpretation:
        # The identified dimensionless group is Pi = etaP^{-2.0} * Vs^{1.0} * r0^{3.0} * alpha^{1.0} * rho^{2.0} * Cp^{2.0} * Tl-T0^{2.0}.
        # The negative exponent on etaP (effective laser power) indicates that as laser power increases, Pi decreases.
        # The regression slope (m) is -0.6440, meaning e* is inversely proportional to Pi^(0.6440).
        # Therefore, increasing laser power (which decreases Pi) leads to an increase in e* (keyhole aspect ratio),
        # which is physically intuitive as higher power generally leads to deeper penetration.
        # The high exponents on r0 (laser beam radius), rho (material density), Cp (heat capacity),
        # and Tl-T0 (temperature difference) suggest a strong non-linear dependence of the keyhole aspect ratio
        # on these parameters. For instance, r0^3 implies a cubic relationship, which might reflect the volume
        # of material affected or the energy density distribution.
        # This group can be compared with known dimensionless numbers in laser-material interaction,
        # such as the Peclet number (related to Vs, r0, alpha) or dimensionless heat input parameters,
        # to gain deeper physical insight.
        # The normalization by Vs (exponent 1.0) means that the group is structured such that Vs is in the numerator.
        # If b_fixed = -1.0 had yielded a higher R-squared, Vs would be in the denominator, leading to a different physical interpretation.

        # Plotting results
        plot_results(best_r_squared, log_y_data, best_pi_values, best_model, y_data_positive)

    else:
        print("No valid dimensionless quantity found within the specified constraints.")

# Execute the problem-solving function
if __name__ == "__main__":
    solve_dimensional_analysis()
```
****************************************
