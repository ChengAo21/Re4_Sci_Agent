
****************************************
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from fractions import Fraction
from itertools import product

# Technical explanation:
# We perform dimensional analysis to find a dimensionless group Pi formed by the physical quantities:
# etaP, Vs, r0, alpha, rho, cp, (Tl-T0), with exponents (a,b,c,d,e,f,g).
# The dimensionless condition imposes linear constraints on these exponents based on fundamental units:
# Mass (M), Length (L), Time (T), Temperature (Θ).
# We fix the exponent of Vs to 1 (normalization) and enumerate rational exponents for the others within [-3,3].
# For each candidate exponent set, we compute Pi for all data points and fit a linear regression
# between log(Pi) and log(e*). The best R^2 identifies the optimal dimensionless group.

# -------------------------------
# Step 1: Load and preprocess data
# -------------------------------

# File path
file_path = "/home/wyh/LLM_wait/codes/contents_files/dimension_a_case1/dataset_keyhole.csv"

# Read CSV with pandas
df = pd.read_csv(file_path, delim_whitespace=True, engine='python')

# Extract relevant columns by index (0-based):
# Columns 3,4,5,6,7,8,11 correspond to:
# etaP (col 3), Vs (col 4), r0 (col 5), alpha (col 6), rho (col 7), cp (col 8), (Tl-T0) (col 11)
# e* is last column (index -1)
etaP = df.iloc[:, 3].values.astype(float)
Vs = df.iloc[:, 4].values.astype(float)
r0 = df.iloc[:, 5].values.astype(float)
alpha = df.iloc[:, 6].values.astype(float)
rho = df.iloc[:, 7].values.astype(float)
cp = df.iloc[:, 8].values.astype(float)
Tl_T0 = df.iloc[:, 11].values.astype(float)
e_star = df.iloc[:, -1].values.astype(float)

# Stack variables into a matrix Q: shape (n_samples, 7)
Q = np.vstack([etaP, Vs, r0, alpha, rho, cp, Tl_T0]).T

# -------------------------------
# Step 2: Define fundamental dimensions of each quantity
# -------------------------------

# Dimensions: M (mass), L (length), T (time), Θ (temperature)
# Represented as vectors [M, L, T, Θ]

# Physical quantities and their dimensions:
# etaP (effective laser power): Power = Energy/time = ML^2T^-3
# Vs (laser scan speed): velocity = LT^-1
# r0 (laser beam radius): length = L
# alpha (thermal diffusivity): m^2/s = L^2 T^-1
# rho (density): kg/m^3 = M L^-3
# cp (heat capacity): J/(kg·K) = (ML^2T^-2)/(M Θ) = L^2 T^-2 Θ^-1
# Tl_T0 (temperature difference): Θ

# Construct dimension matrix D with shape (4 fundamental dims, 7 quantities)
# Each column corresponds to a quantity's dimension vector

D = np.array([
    # M,   L,    T,    Θ
    [1,    0,    0,    0,    1,    0,    0],   # Mass M
    [2,    1,    1,    2,   -3,    2,    0],   # Length L
    [-3,  -1,    0,   -1,    0,   -2,    0],   # Time T
    [0,    0,    0,    0,    0,   -1,    1]    # Temperature Θ
], dtype=float)

# Explanation of D columns:
# etaP: M=1, L=2, T=-3, Θ=0
# Vs:   M=0, L=1, T=-1, Θ=0
# r0:   M=0, L=1, T=0,  Θ=0
# alpha:M=0, L=2, T=-1, Θ=0
# rho:  M=1, L=-3,T=0,  Θ=0
# cp:   M=0, L=2, T=-2, Θ=-1
# Tl_T0:M=0, L=0, T=0,  Θ=1

# -------------------------------
# Step 3: Dimensional homogeneity constraints
# -------------------------------

# We want exponents vector x = [a,b,c,d,e,f,g] such that:
# D @ x = 0 (dimensionless)

# We fix b = 1 (exponent of Vs) to normalize exponents by Vs.

# So the system becomes:
# D @ x = 0 with x[1] = 1

# Rearrange to solve for other exponents in terms of b=1.

# Let x = [a, b=1, c, d, e, f, g]^T

# Write D @ x = 0:
# D[:,0]*a + D[:,1]*1 + D[:,2]*c + D[:,3]*d + D[:,4]*e + D[:,5]*f + D[:,6]*g = 0

# Rearranged:
# D[:,0]*a + D[:,2]*c + D[:,3]*d + D[:,4]*e + D[:,5]*f + D[:,6]*g = -D[:,1]*1

# Define matrix A for unknowns [a,c,d,e,f,g] and vector b_vec = -D[:,1]

A = D[:, [0, 2, 3, 4, 5, 6]]  # shape (4,6)
b_vec = -D[:, 1]               # shape (4,)

# The system is underdetermined (4 equations, 6 unknowns).
# We have 2 degrees of freedom.

# We will enumerate rational values for two free variables and solve for the rest.

# Choose free variables: d (index 3 in x) and g (index 6 in x)
# So unknowns vector u = [a, c, e, f] depend on chosen d and g.

# Rewrite system as:
# A[:, [0,1,3,4]] @ [a,c,e,f]^T = b_vec - d*A[:,2] - g*A[:,5]

# Indices in A:
# 0:a, 1:c, 2:d, 3:e, 4:f, 5:g

# -------------------------------
# Step 4: Enumerate rational exponents for free variables d and g
# -------------------------------

# Allowed rational exponents: multiples of 0.5 in [-3,3]
allowed_rationals = [Fraction(n, 2) for n in range(-6, 7)]  # -3 to 3 step 0.5

# Convert fractions to float for computation
allowed_vals = np.array([float(frac) for frac in allowed_rationals])

# Prepare to store best results
best_r2 = -np.inf
best_exponents = None
best_pred = None

# Precompute log of physical quantities for regression
# To avoid log(0), filter out zero or negative values (should not occur physically)
if np.any(Q <= 0) or np.any(e_star <= 0):
    raise ValueError("Physical quantities and e* must be positive for log-transform.")

log_Q = np.log(Q)       # shape (n_samples, 7)
log_e_star = np.log(e_star)

# -------------------------------
# Step 5: Grid search over d and g
# -------------------------------

print("Starting grid search over free exponents d and g...")

for d_val, g_val in product(allowed_vals, repeat=2):
    # Right hand side vector
    rhs = b_vec - d_val * A[:, 2] - g_val * A[:, 5]  # shape (4,)

    # Coefficient matrix for [a,c,e,f]
    A_sub = A[:, [0, 1, 3, 4]]  # shape (4,4)

    # Solve linear system A_sub @ x_sub = rhs
    try:
        x_sub = np.linalg.solve(A_sub, rhs)  # [a, c, e, f]
    except np.linalg.LinAlgError:
        # Singular matrix, skip
        continue

    a_val, c_val, e_val, f_val = x_sub

    # Compose full exponent vector x = [a,b,c,d,e,f,g]
    # b = 1 (fixed)
    x_full = np.array([a_val, 1.0, c_val, d_val, e_val, f_val, g_val])

    # Check if all exponents are within bounds [-3,3]
    if np.any(np.abs(x_full) > 3):
        continue

    # Check if exponents are close to rational multiples of 0.5 (for interpretability)
    # We allow a tolerance of 0.05
    def is_close_to_half_fraction(x):
        return np.min(np.abs(x - allowed_vals)) < 0.05

    if not all(is_close_to_half_fraction(xi) for xi in x_full):
        continue

    # Compute dimensionless group Pi for all samples:
    # Pi = product_i Q_i^{x_i}
    # Using logs: log(Pi) = sum_i x_i * log(Q_i)
    log_Pi = log_Q @ x_full  # shape (n_samples,)

    # Linear regression: log(e*) = m * log(Pi) + c
    model = LinearRegression()
    model.fit(log_Pi.reshape(-1, 1), log_e_star)
    r2 = model.score(log_Pi.reshape(-1, 1), log_e_star)

    # Update best if improved
    if r2 > best_r2:
        best_r2 = r2
        best_exponents = x_full.copy()
        best_pred = model.predict(log_Pi.reshape(-1, 1))
        best_model = model

print("Grid search completed.")

# -------------------------------
# Step 6: Rationalize exponents to nearest allowed fractions
# -------------------------------

def rationalize_exponent(x):
    # Round to nearest 0.5 fraction within [-3,3]
    candidates = allowed_vals
    idx = np.argmin(np.abs(candidates - x))
    return candidates[idx]

best_exponents_rational = np.array([rationalize_exponent(xi) for xi in best_exponents])

# Normalize exponents by exponent of Vs (which is 1)
norm_factor = best_exponents_rational[1]
best_exponents_normalized = best_exponents_rational / norm_factor

# -------------------------------
# Step 7: Compute final dimensionless group and regression with rationalized exponents
# -------------------------------

log_Pi_final = log_Q @ best_exponents_normalized
model_final = LinearRegression()
model_final.fit(log_Pi_final.reshape(-1, 1), log_e_star)
r2_final = model_final.score(log_Pi_final.reshape(-1, 1), log_e_star)
pred_final = model_final.predict(log_Pi_final.reshape(-1, 1))
residuals = log_e_star - pred_final

# -------------------------------
# Step 8: Print detailed results
# -------------------------------

print("\nOptimal dimensionless group exponents (normalized by Vs exponent):")
quantities = ['etaP', 'Vs', 'r0', 'alpha', 'rho', 'cp', 'Tl_T0']
for q, exp in zip(quantities, best_exponents_normalized):
    # Print as fraction if close
    frac = Fraction(exp).limit_denominator(8)
    if abs(float(frac) - exp) < 1e-2:
        print(f"  {q}: {frac} ({float(frac):.3f})")
    else:
        print(f"  {q}: {exp:.3f}")

print(f"\nLinear regression model (log-log scale):")
print(f"  log(e*) = {model_final.coef_[0]:.4f} * log(Pi) + {model_final.intercept_:.4f}")
print(f"Coefficient of determination R^2 = {r2_final:.4f}")

print("\nResiduals statistics (log scale):")
print(f"  Mean residual: {np.mean(residuals):.4e}")
print(f"  Std residual:  {np.std(residuals):.4e}")

# -------------------------------
# Step 9: Visualization
# -------------------------------

plt.figure(figsize=(8,6))
plt.scatter(np.exp(log_Pi_final), e_star, color='blue', label='Data')
plt.plot(np.exp(log_Pi_final), np.exp(pred_final), color='red', label='Fit')
plt.xlabel('Dimensionless group Pi')
plt.ylabel('Keyhole aspect ratio e*')
plt.title('Correlation between dimensionless group and keyhole aspect ratio')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

****************************************
