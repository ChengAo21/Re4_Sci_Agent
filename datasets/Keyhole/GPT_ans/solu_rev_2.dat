
****************************************
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from fractions import Fraction
from itertools import product
import os

# Technical explanation:
# This code performs dimensional analysis to identify an optimal dimensionless group Pi,
# formed by combining seven physical quantities with rational exponents normalized by Vs exponent.
# The goal is to find exponents that make Pi dimensionless and maximize the coefficient of determination (R^2)
# in a power-law regression model relating Pi to the keyhole aspect ratio e*.
#
# Algorithm steps:
# 1. Load and validate data from CSV, selecting relevant columns by name.
# 2. Define fundamental dimensions (M, L, T, Θ) of each physical quantity and construct dimension matrix D.
# 3. Impose dimensional homogeneity constraints D @ x = 0, fixing exponent of Vs to 1 for normalization.
# 4. Choose two exponents (d and g) as free variables and enumerate their rational values in [-3,3] with step 0.5.
# 5. For each (d,g), solve the linear system for remaining exponents.
# 6. Filter exponent sets by bounds and closeness to rational multiples of 0.5.
# 7. Compute dimensionless group Pi and perform log-log linear regression with e*.
# 8. Select the exponent set with highest R^2.
# 9. Rationalize and normalize exponents, print detailed results.
# 10. Visualize the correlation between Pi and e* on log-log scale.
#
# Optimizations incorporated:
# - Added NaN and infinite value checks for robustness.
# - Modularized key steps into functions for clarity and reusability.
# - Added progress reporting for grid search.
# - Added epsilon safeguard for log-transform to avoid numerical issues.
# - Used Fraction for rationalization to improve exactness.
# - Added residual plot for regression diagnostics.

# -------------------------------
# Step 1: Load and preprocess data
# -------------------------------

def load_and_validate_data(file_path):
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"Data file not found at path: {file_path}")

    df = pd.read_csv(file_path)

    # Required columns by name
    required_cols = ['etaP', 'Vs', 'r0', 'alpha', 'rho', 'cp', 'Tl-T0', 'e*']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Missing required columns in data: {missing_cols}")

    # Extract columns as numpy arrays of floats
    etaP = df['etaP'].values.astype(float)
    Vs = df['Vs'].values.astype(float)
    r0 = df['r0'].values.astype(float)
    alpha = df['alpha'].values.astype(float)
    rho = df['rho'].values.astype(float)
    cp = df['cp'].values.astype(float)
    Tl_T0 = df['Tl-T0'].values.astype(float)
    e_star = df['e*'].values.astype(float)

    # Check for NaN or infinite values
    arrays = [etaP, Vs, r0, alpha, rho, cp, Tl_T0, e_star]
    for arr, name in zip(arrays, required_cols):
        if np.any(np.isnan(arr)):
            raise ValueError(f"NaN values found in column '{name}'.")
        if np.any(np.isinf(arr)):
            raise ValueError(f"Infinite values found in column '{name}'.")

    # Validate positivity for log-transform (add small epsilon to avoid log(0))
    epsilon = 1e-20
    if np.any(etaP <= 0) or np.any(Vs <= 0) or np.any(r0 <= 0) or np.any(alpha <= 0) or \
       np.any(rho <= 0) or np.any(cp <= 0) or np.any(Tl_T0 <= 0) or np.any(e_star <= 0):
        raise ValueError("All physical quantities and e* must be positive for log-transform.")

    # Stack physical quantities into matrix Q (n_samples x 7)
    Q = np.vstack([etaP, Vs, r0, alpha, rho, cp, Tl_T0]).T

    return Q, e_star, df

# -------------------------------
# Step 2: Define fundamental dimensions matrix D
# -------------------------------

def define_dimension_matrix():
    # Dimensions: M (mass), L (length), T (time), Θ (temperature)
    # Columns correspond to quantities in order: etaP, Vs, r0, alpha, rho, cp, Tl_T0
    # Rows correspond to fundamental dimensions: M, L, T, Θ
    D = np.array([
        # M,   L,    T,    Θ
        [1,    0,    0,    0,    1,    0,    0],   # Mass M
        [2,    1,    1,    2,   -3,    2,    0],   # Length L
        [-3,  -1,    0,   -1,    0,   -2,    0],   # Time T
        [0,    0,    0,    0,    0,   -1,    1]    # Temperature Θ
    ], dtype=float)
    return D

# -------------------------------
# Step 3: Dimensional homogeneity constraints and solving
# -------------------------------

def solve_exponents(D):
    # Exponent vector x = [a, b, c, d, e, f, g] for quantities:
    # [etaP, Vs, r0, alpha, rho, cp, Tl_T0]
    # Fix b = 1 (exponent of Vs) to normalize exponents by Vs.

    # The system: D @ x = 0
    # => D[:,0]*a + D[:,2]*c + D[:,3]*d + D[:,4]*e + D[:,5]*f + D[:,6]*g = -D[:,1]*1

    A = D[:, [0, 2, 3, 4, 5, 6]]  # Coefficients for unknowns [a, c, d, e, f, g]
    b_vec = -D[:, 1]              # Right-hand side vector

    # We choose d and g as free variables.
    # Unknowns to solve for: [a, c, e, f] depend on chosen d and g.

    # Rewrite system:
    # A[:, [0,1,3,4]] @ [a, c, e, f]^T = b_vec - d*A[:,2] - g*A[:,5]

    # Indices in A:
    # 0:a, 1:c, 2:d, 3:e, 4:f, 5:g

    return A, b_vec

# -------------------------------
# Step 4: Grid search over free variables d and g
# -------------------------------

def is_close_to_allowed(x, allowed_vals, tol=0.05):
    return np.min(np.abs(allowed_vals - x)) < tol

def rationalize_exponent(x, allowed_vals):
    idx = np.argmin(np.abs(allowed_vals - x))
    return allowed_vals[idx]

def grid_search_exponents(A, b_vec, log_Q, log_e_star, allowed_vals):
    best_r2 = -np.inf
    best_exponents = None
    best_model = None
    best_log_Pi = None

    # For progress reporting
    total = len(allowed_vals)**2
    count = 0

    for d_val, g_val in product(allowed_vals, repeat=2):
        count += 1
        if count % 50 == 0 or count == total:
            print(f"Grid search progress: {count}/{total} combinations evaluated...", end='\r')

        # Compute right-hand side vector for linear system
        rhs = b_vec - d_val * A[:, 2] - g_val * A[:, 5]  # shape (4,)

        # Coefficient matrix for [a, c, e, f]
        A_sub = A[:, [0, 1, 3, 4]]  # shape (4,4)

        # Solve linear system A_sub @ x_sub = rhs
        try:
            x_sub = np.linalg.solve(A_sub, rhs)  # [a, c, e, f]
        except np.linalg.LinAlgError:
            # Singular matrix, skip this combination
            continue

        a_val, c_val, e_val, f_val = x_sub
        b_val = 1.0  # fixed exponent of Vs

        x_full = np.array([a_val, b_val, c_val, d_val, e_val, f_val, g_val])

        # Check bounds [-3,3]
        if np.any(np.abs(x_full) > 3):
            continue

        # Check closeness to allowed rationals for interpretability
        if not all(is_close_to_allowed(xi, allowed_vals) for xi in x_full):
            continue

        # Compute dimensionless group Pi for all samples: log(Pi) = sum_i x_i * log(Q_i)
        log_Pi = log_Q @ x_full  # shape (n_samples,)

        # Linear regression: log(e*) = m * log(Pi) + c
        model = LinearRegression()
        model.fit(log_Pi.reshape(-1, 1), log_e_star)
        r2 = model.score(log_Pi.reshape(-1, 1), log_e_star)

        # Update best if improved
        if r2 > best_r2:
            best_r2 = r2
            best_exponents = x_full.copy()
            best_model = model
            best_log_Pi = log_Pi.copy()

    print()  # for newline after progress
    if best_exponents is None:
        raise RuntimeError("No valid dimensionless group found within specified bounds and rationality constraints.")

    return best_exponents, best_r2, best_model, best_log_Pi

# -------------------------------
# Step 5: Final rationalization and normalization
# -------------------------------

def rationalize_and_normalize_exponents(best_exponents, allowed_vals):
    best_exponents_rational = np.array([rationalize_exponent(xi, allowed_vals) for xi in best_exponents])
    norm_factor = best_exponents_rational[1]  # exponent of Vs fixed to 1
    best_exponents_normalized = best_exponents_rational / norm_factor
    return best_exponents_normalized

# -------------------------------
# Step 6: Regression diagnostics and plotting
# -------------------------------

def regression_diagnostics_and_plot(log_Pi_final, e_star, log_e_star, model_final):
    pred_final = model_final.predict(log_Pi_final.reshape(-1, 1))
    residuals = log_e_star - pred_final

    # Print residual statistics
    print("\nResiduals statistics (log scale):")
    print(f"  Mean residual: {np.mean(residuals):.4e}")
    print(f"  Std residual:  {np.std(residuals):.4e}")

    # Residual plot
    plt.figure(figsize=(8, 4))
    plt.scatter(pred_final, residuals, color='purple', alpha=0.6)
    plt.axhline(0, color='black', linestyle='--', linewidth=1)
    plt.xlabel('Predicted log(e*)')
    plt.ylabel('Residuals')
    plt.title('Residuals of log-log regression')
    plt.grid(True, linestyle='--', linewidth=0.5)
    plt.tight_layout()
    plt.show()

    # Correlation plot
    plt.figure(figsize=(8, 6))
    plt.scatter(np.exp(log_Pi_final), e_star, color='blue', label='Data')
    plt.plot(np.exp(log_Pi_final), np.exp(pred_final), color='red', label='Fit')
    plt.xscale('log')
    plt.yscale('log')
    plt.xlabel('Dimensionless group Pi (log scale)')
    plt.ylabel('Keyhole aspect ratio e* (log scale)')
    plt.title('Correlation between dimensionless group Pi and keyhole aspect ratio e*')
    plt.legend()
    plt.grid(True, which="both", ls="--", linewidth=0.5)
    plt.tight_layout()
    plt.show()

# -------------------------------
# Step 7: Detailed results printing
# -------------------------------

def print_results(best_exponents_normalized, model_final, r2_final):
    quantities = ['etaP', 'Vs', 'r0', 'alpha', 'rho', 'cp', 'Tl_T0']
    print("Optimal dimensionless group exponents (normalized by Vs exponent = 1):")
    for q, exp in zip(quantities, best_exponents_normalized):
        frac = Fraction(exp).limit_denominator(8)
        if abs(float(frac) - exp) < 1e-2:
            print(f"  {q}: {frac} ({float(frac):.3f})")
        else:
            print(f"  {q}: {exp:.3f}")

    print(f"\nLinear regression model (log-log scale):")
    print(f"  log(e*) = {model_final.coef_[0]:.4f} * log(Pi) + {model_final.intercept_:.4f}")
    print(f"Coefficient of determination R^2 = {r2_final:.4f}")

# -------------------------------
# Main execution function
# -------------------------------

def main():
    file_path = "/home/wyh/LLM_wait/codes/contents_files/dimension_a_case1/dataset_keyhole.csv"

    print("Loading and validating data...")
    Q, e_star, df = load_and_validate_data(file_path)

    print("Data preview:")
    print(df.head())
    print(f"\nColumns in data:\n{df.columns.tolist()}")
    print(f"Data shape: {df.shape}\n")

    D = define_dimension_matrix()
    A, b_vec = solve_exponents(D)

    # Allowed rational exponents: multiples of 0.5 in [-3,3]
    allowed_rationals = [Fraction(n, 2) for n in range(-6, 7)]  # -3 to 3 step 0.5
    allowed_vals = np.array([float(frac) for frac in allowed_rationals])

    # Precompute logs for regression with epsilon safeguard
    epsilon = 1e-20
    log_Q = np.log(Q + epsilon)       # shape (n_samples, 7)
    log_e_star = np.log(e_star + epsilon)

    print("Starting grid search over free exponents d and g...")
    best_exponents, best_r2, best_model, best_log_Pi = grid_search_exponents(A, b_vec, log_Q, log_e_star, allowed_vals)
    print("Grid search completed.\n")

    best_exponents_normalized = rationalize_and_normalize_exponents(best_exponents, allowed_vals)

    # Final regression with rationalized exponents
    log_Pi_final = log_Q @ best_exponents_normalized
    model_final = LinearRegression()
    model_final.fit(log_Pi_final.reshape(-1, 1), log_e_star)
    r2_final = model_final.score(log_Pi_final.reshape(-1, 1), log_e_star)

    print_results(best_exponents_normalized, model_final, r2_final)

    regression_diagnostics_and_plot(log_Pi_final, e_star, log_e_star, model_final)

if __name__ == "__main__":
    main()
```
****************************************
