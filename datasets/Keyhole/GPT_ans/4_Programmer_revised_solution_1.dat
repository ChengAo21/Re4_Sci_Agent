
****************************************
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from fractions import Fraction
from itertools import product
import os

# Technical explanation:
# This code performs dimensional analysis to identify an optimal dimensionless group Pi,
# formed by combining seven physical quantities with rational exponents normalized by Vs exponent.
# The goal is to find exponents that make Pi dimensionless and maximize the coefficient of determination (R^2)
# in a power-law regression model relating Pi to the keyhole aspect ratio e*.
#
# Steps:
# 1. Load and validate data from CSV, selecting relevant columns by name to avoid indexing errors.
# 2. Define fundamental dimensions (M, L, T, Θ) of each physical quantity and construct dimension matrix D.
# 3. Impose dimensional homogeneity constraints D @ x = 0, fixing exponent of Vs to 1 for normalization.
# 4. Identify free variables (d and g), enumerate their rational values in [-3,3] with step 0.5.
# 5. Solve linear system for remaining exponents for each candidate (d,g).
# 6. Filter exponent sets by bounds and closeness to rational multiples of 0.5.
# 7. Compute dimensionless group Pi and perform log-log linear regression with e*.
# 8. Select the exponent set with highest R^2.
# 9. Rationalize and normalize exponents, print detailed results.
# 10. Visualize the correlation between Pi and e* on log-log scale.

# -------------------------------
# Step 1: Load and preprocess data
# -------------------------------

file_path = "/home/wyh/LLM_wait/codes/contents_files/dimension_a_case1/dataset_keyhole.csv"

if not os.path.isfile(file_path):
    raise FileNotFoundError(f"Data file not found at path: {file_path}")

# Attempt to read CSV with default delimiter (comma)
df = pd.read_csv(file_path)

# Print basic info to verify data loading
print("Data preview:")
print(df.head())
print("\nColumns in data:")
print(df.columns.tolist())
print(f"Data shape: {df.shape}\n")

# Required columns by name (as per problem statement and sample data)
required_cols = ['etaP', 'Vs', 'r0', 'alpha', 'rho', 'cp', 'Tl-T0', 'e*']

# Check presence of required columns
missing_cols = [col for col in required_cols if col not in df.columns]
if missing_cols:
    raise ValueError(f"Missing required columns in data: {missing_cols}")

# Extract columns as numpy arrays of floats
etaP = df['etaP'].values.astype(float)
Vs = df['Vs'].values.astype(float)
r0 = df['r0'].values.astype(float)
alpha = df['alpha'].values.astype(float)
rho = df['rho'].values.astype(float)
cp = df['cp'].values.astype(float)
Tl_T0 = df['Tl-T0'].values.astype(float)
e_star = df['e*'].values.astype(float)

# Stack physical quantities into matrix Q (n_samples x 7)
Q = np.vstack([etaP, Vs, r0, alpha, rho, cp, Tl_T0]).T

# Validate positivity for log-transform
if np.any(Q <= 0):
    raise ValueError("All physical quantities must be positive for log-transform.")
if np.any(e_star <= 0):
    raise ValueError("Keyhole aspect ratio e* must be positive for log-transform.")

# -------------------------------
# Step 2: Define fundamental dimensions matrix D
# -------------------------------

# Dimensions: M (mass), L (length), T (time), Θ (temperature)
# Columns correspond to quantities in order: etaP, Vs, r0, alpha, rho, cp, Tl_T0
# Rows correspond to fundamental dimensions: M, L, T, Θ

D = np.array([
    # M,   L,    T,    Θ
    [1,    0,    0,    0,    1,    0,    0],   # Mass M
    [2,    1,    1,    2,   -3,    2,    0],   # Length L
    [-3,  -1,    0,   -1,    0,   -2,    0],   # Time T
    [0,    0,    0,    0,    0,   -1,    1]    # Temperature Θ
], dtype=float)

# -------------------------------
# Step 3: Dimensional homogeneity constraints
# -------------------------------

# Exponent vector x = [a, b, c, d, e, f, g] for quantities:
# [etaP, Vs, r0, alpha, rho, cp, Tl_T0]
# Fix b = 1 (exponent of Vs) to normalize exponents by Vs.

# The system: D @ x = 0
# => D[:,0]*a + D[:,1]*1 + D[:,2]*c + D[:,3]*d + D[:,4]*e + D[:,5]*f + D[:,6]*g = 0
# Rearranged:
# D[:,0]*a + D[:,2]*c + D[:,3]*d + D[:,4]*e + D[:,5]*f + D[:,6]*g = -D[:,1]

A = D[:, [0, 2, 3, 4, 5, 6]]  # Coefficients for unknowns [a, c, d, e, f, g]
b_vec = -D[:, 1]              # Right-hand side vector

# We choose d and g as free variables.
# Unknowns to solve for: [a, c, e, f] depend on chosen d and g.

# Rewrite system:
# A[:, [0,1,3,4]] @ [a, c, e, f]^T = b_vec - d*A[:,2] - g*A[:,5]

# Indices in A:
# 0:a, 1:c, 2:d, 3:e, 4:f, 5:g

# -------------------------------
# Step 4: Enumerate rational exponents for free variables d and g
# -------------------------------

# Allowed rational exponents: multiples of 0.5 in [-3,3]
allowed_rationals = [Fraction(n, 2) for n in range(-6, 7)]  # -3 to 3 step 0.5
allowed_vals = np.array([float(frac) for frac in allowed_rationals])

# Prepare to store best results
best_r2 = -np.inf
best_exponents = None
best_model = None
best_log_Pi = None

# Precompute logs for regression
log_Q = np.log(Q)       # shape (n_samples, 7)
log_e_star = np.log(e_star)

# Helper function to check closeness to allowed rationals within tolerance
def is_close_to_allowed(x, allowed=allowed_vals, tol=0.05):
    return np.min(np.abs(allowed - x)) < tol

print("Starting grid search over free exponents d and g...")

for d_val, g_val in product(allowed_vals, repeat=2):
    # Compute right-hand side vector for linear system
    rhs = b_vec - d_val * A[:, 2] - g_val * A[:, 5]  # shape (4,)

    # Coefficient matrix for [a, c, e, f]
    A_sub = A[:, [0, 1, 3, 4]]  # shape (4,4)

    # Solve linear system A_sub @ x_sub = rhs
    try:
        x_sub = np.linalg.solve(A_sub, rhs)  # [a, c, e, f]
    except np.linalg.LinAlgError:
        # Singular matrix, skip this combination
        continue

    a_val, c_val, e_val, f_val = x_sub

    # Compose full exponent vector x = [a,b,c,d,e,f,g]
    b_val = 1.0  # fixed
    x_full = np.array([a_val, b_val, c_val, d_val, e_val, f_val, g_val])

    # Check bounds [-3,3]
    if np.any(np.abs(x_full) > 3):
        continue

    # Check closeness to allowed rationals for interpretability
    if not all(is_close_to_allowed(xi) for xi in x_full):
        continue

    # Compute dimensionless group Pi for all samples: log(Pi) = sum_i x_i * log(Q_i)
    log_Pi = log_Q @ x_full  # shape (n_samples,)

    # Linear regression: log(e*) = m * log(Pi) + c
    model = LinearRegression()
    model.fit(log_Pi.reshape(-1, 1), log_e_star)
    r2 = model.score(log_Pi.reshape(-1, 1), log_e_star)

    # Update best if improved
    if r2 > best_r2:
        best_r2 = r2
        best_exponents = x_full.copy()
        best_model = model
        best_log_Pi = log_Pi.copy()

print("Grid search completed.\n")

if best_exponents is None:
    raise RuntimeError("No valid dimensionless group found within specified bounds and rationality constraints.")

# -------------------------------
# Step 5: Rationalize exponents to nearest allowed fractions
# -------------------------------

def rationalize_exponent(x):
    # Round to nearest allowed rational fraction within [-3,3]
    idx = np.argmin(np.abs(allowed_vals - x))
    return allowed_vals[idx]

best_exponents_rational = np.array([rationalize_exponent(xi) for xi in best_exponents])

# Normalize exponents by exponent of Vs (index 1)
norm_factor = best_exponents_rational[1]
best_exponents_normalized = best_exponents_rational / norm_factor

# -------------------------------
# Step 6: Final regression with rationalized exponents
# -------------------------------

log_Pi_final = log_Q @ best_exponents_normalized
model_final = LinearRegression()
model_final.fit(log_Pi_final.reshape(-1, 1), log_e_star)
r2_final = model_final.score(log_Pi_final.reshape(-1, 1), log_e_star)
pred_final = model_final.predict(log_Pi_final.reshape(-1, 1))
residuals = log_e_star - pred_final

# -------------------------------
# Step 7: Print detailed results
# -------------------------------

print("Optimal dimensionless group exponents (normalized by Vs exponent = 1):")
quantities = ['etaP', 'Vs', 'r0', 'alpha', 'rho', 'cp', 'Tl_T0']
for q, exp in zip(quantities, best_exponents_normalized):
    frac = Fraction(exp).limit_denominator(8)
    if abs(float(frac) - exp) < 1e-2:
        print(f"  {q}: {frac} ({float(frac):.3f})")
    else:
        print(f"  {q}: {exp:.3f}")

print(f"\nLinear regression model (log-log scale):")
print(f"  log(e*) = {model_final.coef_[0]:.4f} * log(Pi) + {model_final.intercept_:.4f}")
print(f"Coefficient of determination R^2 = {r2_final:.4f}")

print("\nResiduals statistics (log scale):")
print(f"  Mean residual: {np.mean(residuals):.4e}")
print(f"  Std residual:  {np.std(residuals):.4e}")

# -------------------------------
# Step 8: Visualization
# -------------------------------

plt.figure(figsize=(8, 6))
plt.scatter(np.exp(log_Pi_final), e_star, color='blue', label='Data')
plt.plot(np.exp(log_Pi_final), np.exp(pred_final), color='red', label='Fit')
plt.xscale('log')
plt.yscale('log')
plt.xlabel('Dimensionless group Pi (log scale)')
plt.ylabel('Keyhole aspect ratio e* (log scale)')
plt.title('Correlation between dimensionless group Pi and keyhole aspect ratio e*')
plt.legend()
plt.grid(True, which="both", ls="--", linewidth=0.5)
plt.tight_layout()
plt.show()
```

****************************************
